<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 4 Creation of an Application to Capture the Metadata of Big Data | Metadata in the Margins</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content=" 4 Creation of an Application to Capture the Metadata of Big Data | Metadata in the Margins" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="ChantalMB/MitM-MRE-2023" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 4 Creation of an Application to Capture the Metadata of Big Data | Metadata in the Margins" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Chantal M Brousseau" />


<meta name="date" content="2023-08-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="marginalia-in-the-archive.html"/>
<link rel="next" href="case-study-finding-early-modern-marginalia.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Metadata in the Margins</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome to my MRE</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#acknowledgements"><i class="fa fa-check"></i><b>2.1</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="marginalia-in-the-archive.html"><a href="marginalia-in-the-archive.html"><i class="fa fa-check"></i><b>3</b> Marginalia in the Archive</a>
<ul>
<li class="chapter" data-level="3.1" data-path="marginalia-in-the-archive.html"><a href="marginalia-in-the-archive.html#the-historiography-of-marginalia"><i class="fa fa-check"></i><b>3.1</b> The Historiography of Marginalia</a></li>
<li class="chapter" data-level="3.2" data-path="marginalia-in-the-archive.html"><a href="marginalia-in-the-archive.html#presence-in-physical-and-digital-archives"><i class="fa fa-check"></i><b>3.2</b> Presence in Physical and Digital Archives</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="marginalia-in-the-archive.html"><a href="marginalia-in-the-archive.html#what-is-marginalia"><i class="fa fa-check"></i><b>3.2.1</b> What is Marginalia?</a></li>
<li class="chapter" data-level="3.2.2" data-path="marginalia-in-the-archive.html"><a href="marginalia-in-the-archive.html#physical-and-digital-presence"><i class="fa fa-check"></i><b>3.2.2</b> Physical and Digital Presence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="marginalia-in-the-archive.html"><a href="marginalia-in-the-archive.html#on-object-marks-and-machines"><i class="fa fa-check"></i><b>3.3</b> On Object Marks and Machines</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="creation-of-an-application-to-capture-the-metadata-of-big-data.html"><a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html"><i class="fa fa-check"></i><b>4</b> Creation of an Application to Capture the Metadata of Big Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="creation-of-an-application-to-capture-the-metadata-of-big-data.html"><a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#technical-overview"><i class="fa fa-check"></i><b>4.1</b> Technical Overview</a></li>
<li class="chapter" data-level="4.2" data-path="creation-of-an-application-to-capture-the-metadata-of-big-data.html"><a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#process"><i class="fa fa-check"></i><b>4.2</b> Process</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="creation-of-an-application-to-capture-the-metadata-of-big-data.html"><a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#interface-and-tooling"><i class="fa fa-check"></i><b>4.2.1</b> Interface and Tooling</a></li>
<li class="chapter" data-level="4.2.2" data-path="creation-of-an-application-to-capture-the-metadata-of-big-data.html"><a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#annotation-and-metadata"><i class="fa fa-check"></i><b>4.2.2</b> Annotation and Metadata</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="case-study-finding-early-modern-marginalia.html"><a href="case-study-finding-early-modern-marginalia.html"><i class="fa fa-check"></i><b>5</b> Case Study: Finding Early Modern Marginalia</a>
<ul>
<li class="chapter" data-level="5.1" data-path="case-study-finding-early-modern-marginalia.html"><a href="case-study-finding-early-modern-marginalia.html#the-cultural-heritage-collection-as-data"><i class="fa fa-check"></i><b>5.1</b> The Cultural Heritage Collection as Data</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="case-study-finding-early-modern-marginalia.html"><a href="case-study-finding-early-modern-marginalia.html#nls-chapbooks-in-the-data-foundry"><i class="fa fa-check"></i><b>5.1.1</b> NLS Chapbooks in the Data Foundry</a></li>
<li class="chapter" data-level="5.1.2" data-path="case-study-finding-early-modern-marginalia.html"><a href="case-study-finding-early-modern-marginalia.html#the-archaeology-of-reading"><i class="fa fa-check"></i><b>5.1.2</b> The Archaeology of Reading</a></li>
<li class="chapter" data-level="5.1.3" data-path="case-study-finding-early-modern-marginalia.html"><a href="case-study-finding-early-modern-marginalia.html#early-modern-annotated-books"><i class="fa fa-check"></i><b>5.1.3</b> Early Modern Annotated Books</a></li>
<li class="chapter" data-level="5.1.4" data-path="case-study-finding-early-modern-marginalia.html"><a href="case-study-finding-early-modern-marginalia.html#dataset-provenance"><i class="fa fa-check"></i><b>5.1.4</b> Dataset Provenance</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="case-study-finding-early-modern-marginalia.html"><a href="case-study-finding-early-modern-marginalia.html#annotation-process-and-dataset-formation"><i class="fa fa-check"></i><b>5.2</b> Annotation Process and Dataset Formation</a></li>
<li class="chapter" data-level="5.3" data-path="case-study-finding-early-modern-marginalia.html"><a href="case-study-finding-early-modern-marginalia.html#the-machine-learning-model"><i class="fa fa-check"></i><b>5.3</b> The Machine Learning Model</a></li>
<li class="chapter" data-level="5.4" data-path="case-study-finding-early-modern-marginalia.html"><a href="case-study-finding-early-modern-marginalia.html#results"><i class="fa fa-check"></i><b>5.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>6</b> Conclusions</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Metadata in the Margins</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="creation-of-an-application-to-capture-the-metadata-of-big-data" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number"> 4</span> Creation of an Application to Capture the Metadata of Big Data<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#creation-of-an-application-to-capture-the-metadata-of-big-data" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In 2016, book historian Ryan Cordell called for more robust methods to describe digital artifacts bibliographically within the context of utilizing digitized archives. Research which makes use of these digital objects often fails to account for the sources, technologies, and social realities of the objects’ creation in ways that make their affordances and limitations more readily visible and available for critique.<span class="citation"><a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a></span><span class="citation"><a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a></span> Likewise, in conceptualizing digital archives as sources of data in their book, <em>Data Feminism</em>, Catherine D’Ignazio and Laura Klein discuss data science and ethics as informed by intersectional feminism; they continuously emphasize the necessity of further context at all stages of working with “data”, from acquisition to analysis, because the context which data is situated in is seen as essential to the ultimate “framing and communication of results” formed through its use.<span class="citation"><a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a></span></p>
<p>When archival materials are integrated into research utilizing more traditional methods of historical inquiry, the subject being analyzed tends to be singular, which in turn makes answering these questions manageable without extensive organization, since there is cohesion across sources. Comparatively, the large amount of data needed for machine learning tasks often results in these questions being difficult to answer on a microscopic level, because the metadata for each archival item used is omitted during the process of data collection. The scale of the dataset produced makes such detailed information be perceived as unnecessary. Yet these details which go into the first step of building a machine learning model are vital to understanding the influences and limitations of it; the foundations which machines learn with and from are human, meaning that they contain “human subjectivities, biases, and distortions” like all other works created by humans.<span class="citation"><a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a></span> In order to produce meaningful output using either analog or automated research methods such as machine learning for identification, it is vital to interrogate the input that contributed to the making of the method being applied for answers regarding social, cultural, historical, institutional, and material conditions under which that input was produced, as well as about the identities of the people who created it.<span class="citation"><a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a></span></p>
<p>These discussions on metadata’s presence within both digital archives and the process of machine learning are what framed the first technical component of my MRE, a further adaptation of an application I created for the International Space Station Archaeology Project (ISSAP) to support the needs of archeologists working on the project.<span class="citation"><a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a></span> The ISSAP version of the application is used to analyze photos taken of the living quarters ISSAP received from the International Space Station. The project sought to understand how astronauts use the space of the habitation modules by tracking the small items of daily use across the station as they appeared and disappeared in photographs.<span class="citation"><a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a></span> The tool I created for ISSAP is a rewritten version of the more general purpose Visual Geometry Group Image Annotator developed at Oxford University; the original tool was not suitable for the project because it could not be used collaboratively and did not have a structure for the automated recording of metadata as annotations are generated.<span class="citation"><a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a></span> In general, image annotators are used when creating applications for computer vision to create datasets for the purpose of training classification or detection [based] machine learning models to recognize items of interest. The researcher annotates the image, and the machine learning model learns to look for these features which have been marked as important. A side effect of this approach is that the metadata created by the researcher while producing these annotations to train the model with becomes divorced from the original metadata of the images. When considering this in relation to the study of book marginalia, it is equivalent to cutting the marginalia out of the pages and only analyzing those select segments without any thought about the document the marginalia came from.</p>
<p>I sought to expand on the collaboration functionality as well as the ability to import existing metadata from both the archive and directly from the images to be annotated. With this feature, new metadata can be produced alongside the context of the original object metadata common in archives, allowing for the tool to become not only an image annotator, but also a way to reference and track the creation of training data for machine learning projects. Additionally, all metadata will be searchable, allow for researchers to easily reference specific data points and for scholars to browse the data which formed the output being presented to them.</p>
<div id="technical-overview" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Technical Overview<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#technical-overview" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>[Before delving into the details of its creation, it is important to briefly consider the technical foundation of the application, not only for the purposes of transparency and reproducabilty, but also to understand the very first considerations that went into how the end product took form.] The code for this application was written using SvelteKit, a framework for building web applications using a specialised implementation of JavaScript.[<a href="https://kit.svelte.dev/" class="uri">https://kit.svelte.dev/</a>] When the website is compiled, the step in the web development process where the code is converted into what is displayed on a web page, the code is converted into highly efficient vanilla JavaScript resulting in faster performance [compared to traditional frameworks that compile into multifaceted layers]. It is this performance advantage, as well as easy of use, that led me to select SvelteKit for this task, since as an image annotator, users would be uploading and interacting with with files, as well as [populating files with data] which can be computationally heavy tasks, thus having a performant framework was a must.</p>
<p>To transform the SvelteKit web application into a desktop application, I used Electron, an open-source software framework that allows developers to build cross-platform desktop applications using web technologies such as HTML, CSS, and JavaScript. Electron powers many popular desktop applications such as Discord, Slack, Notion, and even the application which I am writing in right now, Visual Studio Code. [It functions by running the web application code using a Chromium browser engine, essentially turning the code into a browser itself designed to do the singular task which the web application’s code instructs it to do.] Due to its popularity, Electron has extensive documentation and a large community making it a good choice for developers such as myself who have little experience creating desktop-based software. The most significant limitation Electron presents is that by using web technologies to build desktop applications, there can be a slightly higher memory consumption and application size compared to “native” applications, which are software that is developed specifically for a particular operating system or platform and thus very able to be more optimized [since they are developed with very specific hardware parameters in mind]. Alternatives to Electron which focus on reducing memory consumption and application size have begun to be developed in recent years, the most notable being Tauri, however they optimize the application through using whatever browser engine the operating system comes pre-installed with rather than installing a dedicated one; for example, if a user is opening the application on an Apple device, it will run using a Safari browser engine. Each browser has different development standards and ways which they display information, so if the application uses the browser engine based on the user’s device, the developer ultimately has little control over how the application appears and functions outside of the operating system that they use to develop the application, unless they have access to multiple machines to perform testing, as well as the resources to tailor a version of the application to each browser engine.</p>
<p>I chose to make my application a desktop tool rather than publish it as a website to ensure both ease of collaboration and use. The application functions by creating and updating a project save file, which is simply a JSON file that contains all data surrounding the images selected for annotation, as well as any annotations drawn upon each image. JSON files are human readable, common and thus have many tools that make them useable even outside of the application, and small in size which makes them easily shareable. When used in a project with collaborative annotation needs, the project save file can be placed in a shared code repository such as GitHub and versions can be managed using Git. This method also adds a level to transparency to a project using my application, as each step of annotating images and changes are being recorded through each push to the repository, assuming the repository is public. As a security measure, web browsers are not allowed access to a user’s file system; when uploading a file to a website, a temporary [fake] path to the selected file is generated, and this is either used to make a copy of the file which is then stored on the website’s server (for example, Google Drive), or temporarily stored then discarded once the web page is closed, the latter of these options being very resource intensive if uploading a large number of files. Electron applications allow access to the file system, since although it makes use of a browser engine, this engine is installed and run locally on the user’s device rather than being connected to the World Wide Web. When starting a new or existing project with my application, the user is first prompted to select the folder of images which they want to annotate. This establishes a path to where the images are on the user’s computer since this does not get saved in the project save file, as paths to where the images are located are unique to each device. If the user wants to open an existing project, they will also be prompted to select the project save JSON file. The path to this file will also be saved so that when the user manually saves their project, this same file will be updated. Access to the file system also allows for the application to autosave the project save file, so should anything go wrong, the user will lose at most ten minutes of work. In summary, Electron ensures that the functional, visual, and file-based user experience is universal when using this tool.</p>
</div>
<div id="process" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Process<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#process" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="interface-and-tooling" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Interface and Tooling<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#interface-and-tooling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As a whole, when designing the interface of my application, I sent through the design around the user experience/design concepts of mapping, and the principle of familiarity. The principle of familiarity is concerned with the ability of an interactive system to allow a user to map prior experiences, either real world or gained from interaction with other systems, onto the features of a new system. By extension, mapping in this context is using a familiar imagery to invoke the action/operation which an interactive element will perform. The layout of the app is similar to other popular tools for image manipulation, such as those within the Adobe Suite, MS Paint, or Windows Photo Viewer.</p>
<p>[screenshot comparing app to PS]</p>
<p>To the left of the window is a simple tool bar, which allows user to select the shape they want to use for annotating the image (set to a rectangle by default), perform basic manipulations like zooming in and out in a controlled manner, and “reset” the image to its original position. This tool bar is hovering over the largest component of the application, the image viewer and annotation canvas. This viewer utilizes technology that those in the humanities are likely already familiar with, [even if they might not be aware of it]. The image viewer itself uses OpenSeadragon, a tool for viewing high-resolution zoomable images, which is the technology largely behind image viewers used by digital archives. aside from using the tool bar’s buttons, OpenSeadragon also allows user to manipulate the image using trackpad gestures as well as click-and-drag to move around the image. Annotorious works with OpenSeadragon to allow for annotations to be drawn on images viewed in the OpenSeadragon window; this combination has been leveraged for cultural heritage purposes [before], one notable example being the Arts and Humanities Research Council crowdsourcing platform, MicroPasts, which allows for the public to assist with [large scale] archaeology, history and heritage tasks.[<a href="https://crowdsourced.micropasts.org/" class="uri">https://crowdsourced.micropasts.org/</a>] At the bottom right of the image viewer are arrows the user may use to switch from one image to the next, however they may also do so by using the left and right arrow keys.</p>
<p>Occupying the right side of the application window is the [primary space for file management]. At the top of this space, users can either return to the home menu should they want to begin a new project, choose to manually save their project, or select where they want the project save file to be saved. Below this is a file viewer, where users can add images they want to annotate from the image folder they selected when creating the project or remove them, as well as view a list of image files they have uploaded. The user can also jump to any of the images by clicking the relevant file name in the file list. Below the file list is a drop down menu where the user may apply a filter that indicates whether images have or have not been annotated, which functions by highlighting the entry in the file list that matches the filter criteria. The search bar functions in the same highlighting manner, except it highlights the images which have metadata that matches the search term. Following this, there is an “Export” menu in which users can choose to export their annotated images [into a variety of popular data formats] used for training object detection models. The last section included in this [side bar] is a quick guide to how the application functions as a reminder to users who have just begun using the application or are returning to the project after a period time away from it.</p>
</div>
<div id="annotation-and-metadata" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Annotation and Metadata<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#annotation-and-metadata" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>[screenshot showing anno/metadata viewer]</p>
<p>The most [academically driven] component of this application is the annotation editor and metadata viewer which occupies most of the application’s bottom pane. [At the surface level], it is designed to appear similar to a spread sheet such as those found in Excel, so it is intuitive to the user understanding what the section of the application is for and how it is used. In the first tab, “Annotations”, there are five descriptive columns present [automatically]: the annotation’s unique ID, the date and time the annotation is created, who the annotation is created by, the broader category the annotations falls into, and what specifically the annotation is. There is a “+” symbol at the end of the column headers that allows the user extend this metadata through adding their own columns specific to the project. A row is added to this table each time an annotation is drawn on an image, and likewise, deleting an annotation on the image canvas deletes the corresponding row, facilitating a direct [connection] between the image and the metadata being generated. The second tab, “Metadata”, displays data associated with the image being annotated– this can be metadata from the digital archive which the image was obtained from if the archive chooses to tag their images with this information or the user does so in the process of collecting the images, as well as any additional EXIF data that can be extracted from the image.</p>
<p>Returning to Cordell’s work, he encourages [us] to think of items found within digital archives as not simply a transparent surrogate for a corresponding physical object, but instead as a “new edition” in the full bibliographic sense of the word; while it “departs more and more from the form impressed upon it by its original author,” it nonetheless “exerts, through its imperfections as much as through its perfections, its own influence upon its surroundings.”<span class="citation"><a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a></span> When it comes to cultural heritage collections, the digtised item is often described in metadata as if it were the original item picture rather than a new version; [in museums, replicas of detoriated artefacts are marked as such, yet digitized objects are often treated as if they are exact subsitutes for the physical.] As Adam Crymble demonstrates in his history of mass digitization, the digitization of primary sources was to a great extent driven by the desire to democratize primary sources for education and research purposes; in the beginning, digitised sources <em>were</em> explicitly intended to be surrogates for the original.<span class="citation"><a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a></span> By and large, digitized sources have been used as such, and so this form of metadata has been considered suitable for its audience. Yet in the age of big data and machine learning, the digitial archive’s audience has shifted from solely human consumption to machine consumption as well, and [archival metadata and what that entails] must be expanded to fit this use. [Metadata, the data which describes data, is what holds data accountable.]</p>
<p>In [the context of machine learning], what has been perceived as valuble is the data that will be used to train a model, and any data surrounding that data is largely ignored or discarded after it is finished its use in create the training data. A model uses an algorithim to make sense of the data given to it, and produce some form of task or output, such as classifying images or generating a paragraph of text. In order for an algorithim to adquately “learn” to do something, it needs an extensive number of examples; for example, the Common Objects in Context (COCO) dataset is a popular dataset used for training object detection models, and it contains 1.5 million examples of objects in photos which fall into one of 80 categories.[<a href="https://cocodataset.org/#home" class="uri">https://cocodataset.org/#home</a>] It is thought that with such expansive datasets, what purpose would such [microscopic] information serve when the data is [not created by nor possible to be revised in its entirety by a person]? This line of thought has resulted in researchers lacking an understanding of the training data being used to train their models, which has led to multiple instances of machines learning to replicate the harmful views their creators possess. [give example of this].</p>
<p>In recent years, there has been movement within the field of computer science towards critical analysis of how datasets are constructed, composed, and used. Primarily, these efforts have been directed toward standardizing the documentation of datasets through “datasheets”, overviews attached to datasets which communicate the content of a dataset in a way that prioritizes transparency and accountability.<span class="citation"><a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a></span> Within this conversation, there has been encouragement to draw upon the existing language and procedures for managing sociocultural data within libraries and archives. In <em>Lessons from Archives</em>, scholars Eun Seo Jo and Timnit Gebru argue that archives, as “a form of large-scale, collective human record-keeping”, can aid in addressing the questions of power imbalance, privacy, and other ethical concerns that datasheets leave unaddressed [through interventionist data collection strategies to address biases and ensure fair representation].<span class="citation"><a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a></span> They indicate a number of ways they believe practices which emerge from archival studies would enhance the practice of machine learning; firstly, that archives begin with focused, institutional mission statements that outline a commitment to “collecting the cultural remains of certain concepts, topics, or demographic groups” which guides their data collection process, as well as curators who are responsible for weighing the risks and benefits of gathering different types of data in relation to an archive’s objectives and have developed theoretical frameworks for appraising collected data.<span class="citation"><a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a></span> Gebru and Jo encourage the machine learning community to approach data collection and appraisal by at least starting with a statement of commitment rather than starting with datasets by availability to ensure equitable targets during the construction of datasets. Further, archives often have codes of conduct or ethics and a professional framework for enforcing them alongside developed detailed standards for data description, ensuring ethical practices in data collection by helping ensure transparency and accountability; these multi-faceted forms of review and record-keeping are unheard of in machine learning data collection.<span class="citation"><a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a></span> Lastly, archival sciences have promoted collective efforts to address issues of representation, inclusivity, and power imbalance; for example, community-based activism has been used to ensure that various cultures are represented in the manner in which they would like to be seen.<span class="citation"><a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a></span> Machine learning researchers can draw from these efforts towards participatory archives to ensure diverse and inclusive datasets.</p>
<p>As <em>Lessons from Archives</em> highlights, the issues of [historical power structures and how they may now be undone] have long been discussed within archival studies. Yet what <em>Lessons from Archives</em> does not discuss is the digital turn within the archive itself, how the archive, through embracing a digital form, has moved from being a [collective of human recordkeeping] to a collection of data to be made sense of and mined.<span class="citation"><a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a></span> When viewing collections themselves as data, archival data is seen as beneficial for the existing metadata associated with/describing each archival item; unlike a blog post where metadata about it needs to be constructed by identifying and compiling available information from the web page, items in a collection have this descriptive information already curated and compiled. Yet a significant issues comes from this perception of digtial archives broadly as [complete] in their current state. Despite instituational mission statements, codes of ethics, and community contributions, at an individual item level, the metadata is still the same as that in catalogs which have long represented groups of people in problematic ways. What has changed is the new methodology being used to promote the use and reuse of these descriptions and collections. As librarian Sophie Ziegler writes in their article <em>Open Data in Cultural Heritage Institutions: Can We Be Better Than Data Brokers?</em>, “The collections as data framework in cultural institutions carries with it the possibility for our descriptions of people to be shared, combined with other data, and used to negatively affect groups.”<span class="citation"><a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a></span> When framing the archive as data, there is a risk that the archival holdings and their descriptions will look objective and natural, and the work of archivists and others to show how archival collections are never neutral and natural will be obscured; Devon Mordell encourages “active participation and critical discourse” around the tools and practices to ensure that new technologies reinscribe this false sense of neutrality.<span class="citation"><a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a></span><span class="citation"><a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a></span> One simple yet significant action that works toward this goal is incorporating data process and provenance into the standardized documentation practices for collections. During his time as Humanities Data Curator at the University of California Santa Barbara, Thomas Padilla emphasized the concept of <em>legibility</em> within metadata, that to make collections as data usable, the processes behind their establishment must be transparent and documented. In the context of libraries, Padilla indicates that:</p>
<blockquote>
<p>Libraries do not often provide access to the scripts that generate collection derivatives, access to processes for cleaning or subsetting data, access to custom schema that have been used, indications of how representative digital holdings are relative to overall holdings, nor is the quality of data typically indicated. Libraries do not typically expose why some collections have been made available and others have not. Libraries do not typically identify the library staff personally responsible for modifying, describing, and creating collections – a dimension of provenance that must be accessed in order to determine data ability to support a research claim.<span class="citation"><a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a></span></p>
</blockquote>
<p>These same claims can be applied to archival items. Without this information, the user’s ability to comprehend and thus utilize a collections as data is hindered which leaves the data vulnerable to misuse. The potential of collections as data hinges on integrity validated through expanded documentation practice.</p>
<p>The annotation editor and metadata viewer within my application seeks to address the digital archive in the state it is at present, where the level at which data provenance is addressed varies widely from institution to institution, in a practical manner. The metadata viewer shows both how the image was contextualise within the archive it was extracted from, and since many archives have not yet [started] to include information about the entry as a digital object, the application includes the automatic extraction of EXIF data from the image to expand the predefined archival metadata. EXIF data is the metadata embedded within digital images, and it can provide details on camera settings including make and model, date and time the image was captured, geographic information regarding where an image was taken, photography settings such as white balance or flash usage, and information on what software was used to process or edit the image. Essentially, a potentially detailed history of how an image was captured and processed when this information might not otherwise be present. Being able to view both the archival and digital metadata in the process of annotation ultimately aides in circumventing the decontextualized access and consumption which occurs during the process of annotating data for computational research.<span class="citation"><a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a></span></p>
<p>In [light of discussion over digitized archival objects being a new edition in the lineage of an item], the annotation editor expands on this mode of thought and encourages the annotator to view their annotations in the same way. Each annotation visually segments a portion of the image from its surroundings, marking it as something significant which is important enough to be highlighted and thus it should be documented in a way that is similar to other digital objects. One way [humanists can distinguish themselves] in the process of creating datasets with the end goal of machine learning is through the addition of explanations about decisions that we make while creating data.<span class="citation"><a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a></span> While the ability to add their own columns in this tab encourage the user to create structured metadata for their annotations, even without adding additional columns, the user still must record who created the annotation and basic descriptive information about the contents of the annotation, capturing key metadata which holds the creator of it accountable in the process of creation for each annotation. Treating annotations as new digital objects both enhances familiarity with the training data and constructs a more robust log of the training data with more findable items should an issue arise during the process of or after training a model.</p>
<p>[Add a blurb about app limitations here or leave that for the README on GitHub?]</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="39">
<li id="fn39"><p><a href="bibliography.html#ref-cordell_q_2017" role="doc-biblioref">Ryan Cordell, <span>“"<span>Q</span> i-Jtb the <span>Raven</span>": <span>Taking</span> <span>Dirty</span> <span>OCR</span> <span>Seriously</span>,”</span> <em>Book History</em> 20, no. 1 (2017): 191, <a href="https://doi.org/10.1353/bh.2017.0006" role="doc-biblioref">https://doi.org/10.1353/bh.2017.0006</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref39" class="footnote-back">↩︎</a></p></li>
<li id="fn40"><p><a href="bibliography.html#ref-mak_archaeology_2014" role="doc-biblioref">Bonnie Mak, <span>“Archaeology of a Digitization,”</span> <em>Journal of the Association for Information Science and Technology</em> 65, no. 8 (2014): 3, <a href="https://doi.org/10.1002/asi.23061" role="doc-biblioref">https://doi.org/10.1002/asi.23061</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref40" class="footnote-back">↩︎</a></p></li>
<li id="fn41"><p><a href="bibliography.html#ref-dignazio_data_2020" role="doc-biblioref">D’Ignazio and Klein, <em>Data <span>Feminism</span></em>, 164</a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref41" class="footnote-back">↩︎</a></p></li>
<li id="fn42"><p><a href="bibliography.html#ref-lee_compounded_2021" role="doc-biblioref">Benjamin Lee, <span>“Compounded <span>Mediation</span>: <span>A</span> <span>Data</span> <span>Archaeology</span> of the <span>Newspaper</span> <span>Navigator</span> <span>Dataset</span>,”</span> <em>Digital Humanities Quarterly</em> 015, no. 4 (December 2021), <a href="http://www.digitalhumanities.org/dhq/vol/15/4/000578/000578.html" role="doc-biblioref">http://www.digitalhumanities.org/dhq/vol/15/4/000578/000578.html</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref42" class="footnote-back">↩︎</a></p></li>
<li id="fn43"><p><a href="bibliography.html#ref-dignazio_data_2020" role="doc-biblioref">D’Ignazio and Klein, <em>Data <span>Feminism</span></em>, 152</a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref43" class="footnote-back">↩︎</a></p></li>
<li id="fn44"><p><a href="bibliography.html#ref-chantalmb_issap_2022" role="doc-biblioref">ChantalMB, <span>“<span>ChantalMB</span>/Issap-Image-Annotator,”</span> February 2022, <a href="https://github.com/ChantalMB/issap-image-annotator" role="doc-biblioref">https://github.com/ChantalMB/issap-image-annotator</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref44" class="footnote-back">↩︎</a></p></li>
<li id="fn45"><p><a href="bibliography.html#ref-graham_recording_2022" role="doc-biblioref">Shawn Graham and Justin Walsh, <span>“Recording Archaeological Data from Space,”</span> <em>International Space Station Archaeological Project</em>, February 2022, <a href="https://issarchaeology.org/how-do-you-get-from-an-astronauts-photo-to-usable-archaeological-data/" role="doc-biblioref">https://issarchaeology.org/how-do-you-get-from-an-astronauts-photo-to-usable-archaeological-data/</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref45" class="footnote-back">↩︎</a></p></li>
<li id="fn46"><p><a href="bibliography.html#ref-dutta_via_2019" role="doc-biblioref">Abhishek Dutta and Andrew Zisserman, <span>“The <span>VIA</span> <span>Annotation</span> <span>Software</span> for <span>Images</span>, <span>Audio</span> and <span>Video</span>,”</span> in <em>Proceedings of the 27th <span>ACM</span> <span>International</span> <span>Conference</span> on <span>Multimedia</span></em>, <span>MM</span> ’19 (New York, NY, USA: Association for Computing Machinery, 2019), 2276–79, <a href="https://doi.org/10.1145/3343031.3350535" role="doc-biblioref">https://doi.org/10.1145/3343031.3350535</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref46" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p><a href="bibliography.html#ref-cordell_q_2017" role="doc-biblioref">Cordell, <span>“"<span>Q</span> i-Jtb the <span>Raven</span>"”</span></a>, with quote from W. W. Greg.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref47" class="footnote-back">↩︎</a></p></li>
<li id="fn48"><p><a href="bibliography.html#ref-crymble_technology_2021" role="doc-biblioref">Adam Crymble, <em>Technology and the <span>Historian</span>: <span>Transformations</span> in the <span>Digital</span> <span>Age</span></em> (Champaign, IL: University of Illinois Press, 2021), 68, <a href="https://doi.org/10.5406/j.ctv1k03s73" role="doc-biblioref">https://doi.org/10.5406/j.ctv1k03s73</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref48" class="footnote-back">↩︎</a></p></li>
<li id="fn49"><p><a href="bibliography.html#ref-gebru_datasheets_2021" role="doc-biblioref">Timnit Gebru et al., <span>“Datasheets for <span>Datasets</span>,”</span> December 2021, 10, <a href="https://doi.org/10.48550/arXiv.1803.09010" role="doc-biblioref">https://doi.org/10.48550/arXiv.1803.09010</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref49" class="footnote-back">↩︎</a></p></li>
<li id="fn50"><p><a href="bibliography.html#ref-jo_lessons_2020" role="doc-biblioref">Eun Seo Jo and Timnit Gebru, <span>“Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning,”</span> in <em>Proceedings of the 2020 <span>Conference</span> on <span>Fairness</span>, <span>Accountability</span>, and <span>Transparency</span></em>, <span>FAT</span>* ’20 (New York, NY, USA: Association for Computing Machinery, 2020), 2, <a href="https://doi.org/10.1145/3351095.3372829" role="doc-biblioref">https://doi.org/10.1145/3351095.3372829</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref50" class="footnote-back">↩︎</a></p></li>
<li id="fn51"><p><a href="bibliography.html#ref-jo_lessons_2020" role="doc-biblioref">Jo and Gebru, <span>“Lessons from Archives,”</span> 5</a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref51" class="footnote-back">↩︎</a></p></li>
<li id="fn52"><p><a href="bibliography.html#ref-jo_lessons_2020" role="doc-biblioref">Jo and Gebru, <span>“Lessons from Archives,”</span> 7</a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref52" class="footnote-back">↩︎</a></p></li>
<li id="fn53"><p><a href="bibliography.html#ref-jo_lessons_2020" role="doc-biblioref">Jo and Gebru, <span>“Lessons from Archives,”</span> 5</a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref53" class="footnote-back">↩︎</a></p></li>
<li id="fn54"><p><a href="bibliography.html#ref-moss_reconfiguration_2018" role="doc-biblioref">Michael Moss, David Thomas, and Tim Gollins, <span>“The <span>Reconfiguration</span> of the <span>Archive</span> as <span>Data</span> to <span>Be</span> <span>Mined</span>,”</span> <em>Archivaria</em>, November 2018, 131, <a href="https://archivaria.ca/index.php/archivaria/article/view/13646" role="doc-biblioref">https://archivaria.ca/index.php/archivaria/article/view/13646</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref54" class="footnote-back">↩︎</a></p></li>
<li id="fn55"><p><a href="bibliography.html#ref-ziegler_open_2020" role="doc-biblioref">S. L. Ziegler, <span>“Open <span>Data</span> in <span>Cultural</span> <span>Heritage</span> <span>Institutions</span>: <span>Can</span> <span>We</span> <span>Be</span> <span>Better</span> <span>Than</span> <span>Data</span> <span>Brokers</span>?”</span> <em>Digital Humanities Quarterly</em> 014, no. 2 (June 2020), <a href="http://www.digitalhumanities.org/dhq/vol/14/2/000462/000462.html" role="doc-biblioref">http://www.digitalhumanities.org/dhq/vol/14/2/000462/000462.html</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref55" class="footnote-back">↩︎</a></p></li>
<li id="fn56"><p><a href="bibliography.html#ref-ziegler_open_2020" role="doc-biblioref">Ziegler, <span>“Open <span>Data</span> in <span>Cultural</span> <span>Heritage</span> <span>Institutions</span>”</span></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref56" class="footnote-back">↩︎</a></p></li>
<li id="fn57"><p><a href="bibliography.html#ref-mordell_critical_2019" role="doc-biblioref">Devon Mordell, <span>“Critical Questions for Archives as (Big) Data,”</span> <em>Archivaria</em> 87 (2019): 156, <a href="https://proxy.library.carleton.ca/login?qurl=https%3A%2F%2Fwww.proquest.com%2Fscholarly-journals%2Fcritical-questions-archives-as-big-data%2Fdocview%2F2518871266%2Fse-2%3Faccountid%3D9894" role="doc-biblioref">https://proxy.library.carleton.ca/login?qurl=https%3A%2F%2Fwww.proquest.com%2Fscholarly-journals%2Fcritical-questions-archives-as-big-data%2Fdocview%2F2518871266%2Fse-2%3Faccountid%3D9894</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref57" class="footnote-back">↩︎</a></p></li>
<li id="fn58"><p><a href="bibliography.html#ref-padilla_collections_2017" role="doc-biblioref">Thomas Padilla, <span>“On a <span>Collections</span> as <span>Data</span> <span>Imperative</span>,”</span> 2017, 3, <a href="https://escholarship.org/uc/item/9881c8sv" role="doc-biblioref">https://escholarship.org/uc/item/9881c8sv</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p><a href="bibliography.html#ref-milligan_we_2020" role="doc-biblioref">Ian Milligan, <span>“We <span>Are</span> <span>All</span> <span>Digital</span> <span>Now</span>: <span>Digital</span> <span>Photography</span> and the <span>Reshaping</span> of <span>Historical</span> <span>Practice</span>,”</span> <em>The Canadian Historical Review</em> 101, no. 4 (2020): 617, https://doi.org/<a href="https://doi.org/10.3138/chr-2020-0023" role="doc-biblioref">https://doi.org/10.3138/chr-2020-0023</a></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p><a href="bibliography.html#ref-ziegler_open_2020" role="doc-biblioref">Ziegler, <span>“Open <span>Data</span> in <span>Cultural</span> <span>Heritage</span> <span>Institutions</span>”</span></a>.<a href="creation-of-an-application-to-capture-the-metadata-of-big-data.html#fnref60" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="marginalia-in-the-archive.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="case-study-finding-early-modern-marginalia.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-app_dev.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
