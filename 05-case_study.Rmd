# Case Study: Finding Early Modern Marginalia

DISCUSS: Data sources, data wrangling, annotation process, training model, technical results, historiographical results 

- Scraping 
  - 

- Annotation and training model as an iterative process
  - Using YOLOv7 object detection model --> most specialised for real-time object detection, which makes it good at object detection even when given a small number of examples to train from 
  - Stage 1: Will manually annotate a small number of images from AOR --> the creators of this digital archive indicated which book pages have annotations, so it was very easy to obtain examples of annotated pages
    - For diversity, I also annotated a small selection of page excerpts from NLS Chapbooks archive which I uncovered in a previous object detection experiment that sought to find the woodcut illustrations present in chapbooks.
  - Stage 2: Transfer learning pt 1-- freeze the backbone feature extraction layers of model and only train head layers which analyze extracted features and predict the presence, location, and class of objects in the image. 
    - 'freeze' refers to a process of fixing the weights of certain layers in the model during training --> by freezing the layers, their weights are not updated during the training process, allowing them to retain the pre-trained knowledge. 
    - Addresses the challenge of a small dataset size by leveraging a pretrained model's backbone layers (which have learned general features from a larger dataset) while only training the head layers --> approach aims to benefit from the pre-existing knowledge captured by the backbone layers while adapting the model to the specific object detection task with limited data.
  - Stage 3: Transfer learning pt 2-- unfreeze backbone feature extraction layers so that all layers can be trained using updated head weights from previous step

Stage 1: Manual Annotation
- Added "Notes" category to make note of any interesting finds
  - Trying to understand what the marginalia in Coalman's Courtship is exactly --> corrections? indicated in the notes of one annotation that marginalia appears to be an alternate phrasing
    - Annotator of J&M's Courtship appears to be making the same corrections --> Same person?
    - Marginalia seems to be "updating" the language used in the story --> perhaps chapbook was created earlier in the EM period and annotator is annotating it later in EM period
    - Despite more "modern" corrections/rephrasing, there are still many pieces of this reader's marginalia that are not comprehensible to me --> this could however, be used to identify a timeframe of when/where the marginalia was composed or who it was composed by
      - Word "auld" is used --> usage of "auld" peaked in 1820s, but also Scottish/Irish/Northern England
- Find myself referencing the image/archival metadata to aid in comprehension of the marginalia itself

Stage 2.0: Training Model Rnd1
- Trial with small number of ~250 annotated images
  - Low mAP @ 0.594 but still pretty good!
  - ...However testing the model on unseen images reveals the flaw of primarily annotating images from AOR
    - Model almost exclusively looks for marginalia on page peripherals since this is where Dee and Harvey annotated the most
    - Need to diversify dataset 
- Trial with small number of ~350 annotated images including Calisphere
  - Somehow the model got worse with more data... time to reconsider object detection model? 
  - No nvm just overfitted, however the confidence scores are still very low...


- While writing, was easily able to refer back to the pages annotated and find examples through using the search function

- Sherman discusses origin of marks in *Used Books* pg 27-28

- Confidence score 0.05 since model used without fragmentation of image can miss tiny details if detection not set to be very sensitive
- YOLOv7 recommends 1500 images per new class

------------
## Model
- Explain model

## Data Preprocessing
- In total, 353 images of early modern book pages were annotated. Added in to this set of images was 20 [negative examples], that is, pages which contain no marginalia. Another advantage of the YOLO algorithim is that there is typically little need for negative examples; in the intial step of dividing the inputted image into a grid then [analyzing] each segment, the model will naturally learn how to identify "no object" since a majority of images will not contain objects in every grid segment. However, when the images being used to train the model are "busy" with the objects being detected possibly being present across the entire page, such as with images of traffic or in our case, with pages of text and profilic annotation, training results can be improved through the intentional addition of negative examples to reduce false positives caused by heavy overlap of desired objects and [background] objects. 
- This set of images was then randomly split using the standard 80/20 training/test ratio, with 298 (~80%) images being used to *train* the model, 37 (~10%) images being used to *validate* the model while it trains, and 38 (~10%) images to test the results of the trained model.
- YOLOv7 recommends 1500 images per new class, so to give this small dataset a fighting chance at successfully detecting marginalia, I applied various transformations to the original training images to create augmented versions of this data. These transformations modified the images in ways that preserve their essential features but introduce variation that can enhance the model's ability to generalize. Not only does data augmentation increase the amount of training data by by generating multiple versions of each original image, but also by presenting the model with different variations of the same image (e.g., different rotations, flips, translations, zoom levels, etc.), data augmentation helps the model learn to recognize important patterns and features that are invariant to those transformations. At the same time, it also helps prevent the model from memorizing specific details of the training data which results in "overfitting"-- [we] want a model that detects meaningful and relevant features rather than memorizing specific examples.
- In the context of this case study, each of the 298 training images were augmented through added noise, added gaussian blur, randomly changed contrast, randomly changed brightness, and randomly changed saturation. This expanded the total training image set to 1788 (298*6).

- The following sections will be shaped by Benjamin Charles Germain Lee's recently proposed framework for using collections as machine learning data.[@lee_collections_2023]
- My dataset for this case study was created using three digital archives, ranging from those being made intentionally friendly to computational use to ones following the [standard] archival model, each of which thus emboding the current conversations being had about digital archives as big data.


The cultural heritage collection as data
Here, a distinction is drawn between the cultural heritage collection being studied and the training dataset being utilized for the machine learning model. For example, a project might utilize a pre-trained model to generate embeddings for a photo collection. In this section, we consider the cultural heritage collection itself; in the section “The Machine Learning Model,” we consider the machine learning model's training data.

Dataset composition:

Who or what is depicted in the dataset? (Gebru et al., 2020)
- The dataset is composed of annotated pages from three digital archives.
  - Initially spurring this project was a small selection of annotated pages from the National Library of Scotland's collection of chapbooks which I had made note of when they were erranously detected as illustrations, during previous work in which I attempted to classify pages in the corpus that contained illustrations. Since the ultimate goal for this case study is to intentionlly identify marginalia in this collection of chapbooks, I chose to incorporate these pages into the training data for both diversity and to provide domain specific examples in hopes of improving the trained model's performance. 
  - The  *Archaeology of Reading* project which was a collaborative effort between the Sheridan Libraries, Centre for Editing Lives and Letters, and Princeton University library that resulted in a corpus of thirty-six fully digitized versions of early printed books annotated by "two of the most dedicated readers of the early modern period: John Dee and Gabriel Harvey".  Considering himself a scholar of science, the marginalia composed by Dee feature tables, charts, and diagrams to make sense of what he read. In contrast, Harvey was a humanist and approached his selection of reading as such with passages highlighted through underlining and notes connecting the text to other works read.
  - A collection of Early Modern annotated books compiled by the William Andrews Clark Memorial Library at the University of California, Los Angeles which span from the 15th century up until the early 19th century.  The marginalia of this collection are the most diverse of these sources, containing the scrawl of not only scholars, but printers and other members of the community as well.  
If the dataset depicts people, are any specific subgroups of people represented? Are any specific individuals personally identifiable? (Gebru et al., 2020)
- N/A
If the dataset depicts people, are any individuals still living? Does this project comply with privacy laws in countries where it will be shared?
- N/A
What medium is the dataset? (image, video, text, web archive, etc.)
- This dataset is composed of scanned pages of books, extracted from GLAM archives through webscraping
How large is the dataset, both in cardinality and in disk storage?
- In total, 353 images of early modern book pages were annotated. 
- For training, added in to this set of images was 20 [negative examples], that is, pages which contain no marginalia.
- This set of images was then randomly split using an 85/15 training/test ratio, with 317 (~85%) images being used to *train* the model, 39 (~10%) images being used to *validate* the model while it trains, and 17 (~5%) images to test the results of the trained model.
- YOLOv7 recommends 1500 images per new class, so to give this small dataset a fighting chance at successfully detecting marginalia, I applied various transformations to the original training images to create augmented versions of this data. These transformations modified the images in ways that preserve their essential features but introduce variation that can enhance the model's ability to generalize. Not only does data augmentation increase the amount of training data by by generating multiple versions of each original image, but also by presenting the model with different variations of the same image (e.g., different rotations, flips, translations, zoom levels, etc.), data augmentation helps the model learn to recognize important patterns and features that are invariant to those transformations. At the same time, it also helps prevent the model from memorizing specific details of the training data which results in "overfitting"-- [we] want a model that detects meaningful and relevant features rather than memorizing specific examples.
- In the context of this case study, each of the 317 training images were augmented through added noise, randomly changed contrast, randomly changed brightness, and randomly changed saturation. This expanded the total training image set to 1585 (317*4).
- Within the greater context of machine learning, this dataset is very small at only 795 MB.
What metadata is available for the dataset items? (Holland et al., 2018)
- During the process of scraping and storing the page scans, the metadata associated with the archival item, usually about the book which the page is apart of, was also scraped and embedded within the image using Python's PIL library. Within my annotation app, the EXIF information is also extracted from each image uploaded. This metadata is viewable in-app, but also within the app's save file, which is simply a JSON file.
Does copyright impact this dataset? If so, how? (Cordell, 2020; Gebru et al., 2020; Jakeway et al., 2020; Padilla, 2018)
- All data used falls in the public domain and is only being used for research purposes. Further, there appears to be no restrictions from any of the archives indicating that the extracted images cannot be shared in a research context, thus the dataset can be made accessible on an unmonetized platform.
Does this dataset pertain to a difficult history? If so, what extra precautions are being taken?
- N/A


Collecting process and curation rationale (language borrowed from Bender and Friedman (2018)):

Who curated the cultural heritage collection from which this dataset is derived?
- The NLS Data Foundry's collection of chapbooks are part of the Lauriston Castle Collection, which is based on the library built up by William Robert Reid (1854-1919), a wealthy Edinburgh businessman who had acquired Lauriston Castle, on the western outskirts of the city, in 1902.[https://digital.nls.uk/catalogues/special-and-named-printed-collections/?id=598] Reid had been assisted in his book collecting by a family friend, John A Fairley, author of several articles on the bibliography of chapbooks. In the course of his researches Fairley had formed a collection of chapbooks, containing around 500 volumes comprising over 5,500 items, which are now also part of the collection. The chapbooks are organised according to the town where they were printed. The majority of them are Scottish but there are also English and Irish volumes in the collection.
- The books that make up the AOR were selected and annotated by a collective of researchers at the Sheridan Libraries, Centre for Editing Lives and Letters, and Princeton University library, however, the lead researchers Earle Havens, Anthony Grafton, and Lisa Jardine likely had the strongest curatorial role.
- The Early Modern Annotated Books collection hosted on Calisphere was curated by the William Andrews Clark Memorial Library, which is administered by UCLA's Center for 17th- & 18th-Century Studies. This rare book and manuscript library specializes in the study of England and the Continent from the Tudor period through the long eighteenth century.[https://calisphere.org/institution/62/collections/]
What organization or institution was the collection created for?
- The NLS Chapbook dataset was created by the National Library of Scotland's Digital Scholarship Service, who publishes machine-readable data collections on the NLS's Data Foundry. 
- The AOR was created as a collaborative research endeavour to consoldate some of the marginalia produced by Gabriel Harvey and John Dee.
- The William Andrews Clark Memorial Library is apart of UCLA's network of libraries.
What funding was utilized (if known)?
- NLS's Data Foundry is a permanant [branch] of their library and thus no specific funding appears to have been secured to produce this data.
- The research and development of AOR was conducted with major funding from the Andrew W. Mellon Foundation.
- The digitization of the Early Modern Annotated Books collection was initially a 2014 pilot project to digitize just ten annotated books from the Clark library, largely shaped by CLIR postdoctoral fellow Philip Palmer. A small grant from the Gladys Krieble Delmas Foundation allowed the ten books to be transcribed through the hiring of three graduate students onto the project.[@palmer_annotated_2018] The further digitzation of annotated books within the Clark's collections was made possible through funding from the National Endowment for the Humanities, which awarded the library a Humanities Collections and Reference Resources Grant in 2017.
What collection process was utilized? (Bender & Friedman, 2018)
- The AOR provides the most extensive and transparent description of their collection process, largely in the form of an article on how to "do" AOR yourself, that is, how to replicate their work using a researcher's own corpus. They do however also summarize the formation of their corpora in brief, by stating that "the composition of the AOR corpora depended on a number of factors, including the costs of digitization, the availability of books, the books owned by partner institutions, and the particular research interests of the scholars involved."[https://archaeologyofreading.org/how-to-do-aor-yourself/]
- Although the NLS does not make their collection process in the context of digitzing materials clear, they do offer information about how the chapbooks which were digtized were aquired. The Lauriston Castle Collection was bequeathed to the Library in 1926 by Mr and Mrs Reid, following the latter's death that year. This bequest also included the Reid Fund, consisting of £70,000, the income from the estate of Mr and Mrs Reid, which subsequently enabled the Library to acquire printed and manuscript items to add to the national collections.[https://digital.nls.uk/catalogues/special-and-named-printed-collections/?id=598]
- The collection process of the William Andrews Clark Memorial Library is opaque; the metadata is largely bibliographic, and some books include a section on provenance, but it is the provenance of the physical item rather than the digtial.
When was the collection assembled? (i.e., when were the photographs taken or ethnographies recorded?)
- All texts in each collection were created largely during the early modern period, with the oldest printed in the late 15th century and the NLS chapbook collection occasionally featuring chapbooks printed in the 19th century.
- On the Data Foundry's listing for the NLS chapbook dataset, there is only a date stamp for its publication in 2019. However, in the book-level METS data, it states that the chapbooks were captured as part of a project to digitise such in 2015.
- According to their data release page, AOR's collection began assembly in November 2015, and was completed in January 2019.
- There is no specific information on the assembly of the Early Modern Annotated Books collection, however based on the funding statement it can be assumed that the books were captured between 2017 until the project's end in October 2018. 
What instruments were utilized to create the collection? (i.e., a recording device, camera, etc.)
- To gain knowledge of what instruments were used to create the NLS chapbook collection, we once again turn to the METS data, which states that each pages was captured using a Nikon D800E, a popular DSLR camera.
- The AOR does not state what technology was used to capture their texts, although they do indicate that the digitization of the books was done primarily in situ by the repositories themselves or through a contract with UCL Digital Media. They do state that they required the images to have a resolution of 600 DPI, so a DSLR camera like that used by the NLS.
- There is no clear information provided on the tools used by the William Andrews Clark Memorial Library for digitization. Looking at the EXIF data extracted by my annotation app, there are tags such as [GPS] common in TIFF images that may point to a camera having been used over something like a scanner.
If people are included, did individuals consent at the time of collection?
- N/A
What were the decision-making processes behind the collection's curation? (Bender & Friedman, 2018)
- According to the METS data, the NLS chapbook's digital form was curated as part of Library activities to make more Scottish collections available online, and chapbooks were selected for this task due to ease as all volumes are the same size. 
- AOR has the most extensive discussion on the curation of their collection, with dedicated essays on the libraries of Harvey and Dee. Although it is thought that Harvey's library once contained up to 4000 books, following his death it was dispersed with his books scattered in private, public, and academic libraries around the world.[https://archaeologyofreading.org/gabriel-harvey-his-library-and-the-aor-corpus/] So, the selection of which of his books to digitzed for the AOR project was in large part a practical endeavour; the first books chosen were the nine in the possession of the Princeton University Library, one of their partnering institutions. In addition these nine books, other titles were added to the Harvey AOR corpus often based on factors such as availability (does the binding allow for the book to be digitized?) and the affordability of digitization within a given repository. The Princeton books alone did not form thematic unity, as much as they reflected Harvey's intellectual interests in topics of warfare, (Roman) history, law, political economy (i.e., husbandry), and linguistics. However, the the inclusion of five other titles alongside those at Princeton allowed for the expansion of the topics and the formation of "clusters" of books: books which thematically overlap, and which may have been read in conjunction with one another, as Harvey enjoyed doing.
  - While Dee's library was also dispersed posthumously, and in part, prior to his death due to financial troubles, he created a detailed catalouge of his books at numerous points in his life, which made his pursuits much more easily traceable.[https://archaeologyofreading.org/john-dee-his-library-and-the-aor-corpus/] Like when constructing Harvey's AOR corpus, factors such as the availability of books and the price charged by the various institutions for their digitization were taken into account, however with more choice being present due to the number of identifiable books annotated by Dee, further decision about what of his library should be digitise relied on intellectual interest. It was decided that primarily, the books selected from Dee should comprise of types and styles of reader interventions that are not represented in the Harvey corpus, as Dee's corpus contains a number of new interventions, including the use of additional symbols, genealogical trees, complex astrological charts, dense tables, and expansive drawings. Additionally, in order to further reflect a variety of reading and annotation strategies, the AOR Dee corpus also includes lightly annotated books such as Euclid's *Elementorum libri XV*, as well as different book formats, ranging from Cicero's *Opera* in folio to Gerhard Dorn's *Chymisticum artificium* in octavo. Lastly, in relation to Dee's library as a whole, as in Harvey's corpus, the books included in Dee's corpus were selected to reflect the various intellectual interests which Dee pursued throughout his life, including mathematics, astrology/astronomy, medieval history, and New World discovery. The ultimate goal when curating both corpora was an act of balance, reflecting the attempt to cover a representative selection of both readers' intellectual interests and the ways in which they interacted with their books. 
- In contrast, there is very little information about the decision-making process made in the curation of Early Modern Annotated Books collection. The choice to digitise annotated books seems to derive from Palmer's CLIR postdoctoral fellowship on the subject of "Manuscript Annotations in Early Modern Printed Books."; the project started from a selection of ten annotated books, which Palmer described as being "representative of the characteristic idiosyncrasy that historicdal readers brought to their material readings of books."[@palmer_annotated_2018]
What is unknown about the collection process and curation rationale?
- The NLS's reasoning for choosing to digitise the chapbooks is very straightforward, and their process of digitisation is made transparent in the METS data included with each chapbook. What is missing, however, from the process of collection and curation is a question of *who*. In consideration of the original, physical collection of chapbooks, it is not known who owned them prior to Fairley. It is not documented who catalogued the chapbooks prior to digitisation, nor following it, and it is not known who then transformed the chapbooks into the form it takes in NLS's Data Foundry.
- The creation of AOR is extensively documented, and the provenance of each phyiscal book digitised is relatively traceable due to the [fame] of their annotators. The central gap of note in this collection is the process of digitization. This is likely owing to the fact that there were multiple institutions involved in creating the corpora, each having their own digitization process, personnel, and equipment.
- The Early Modern Annotated Books collection is the collection which leaves the most unknowns, and this seems to be at least in part due to the focus on the digitized books as being surrogates for the physical object, rather than a "new edition".[@cordell_q_2017] The metadata associated with each book appears to be about the physical book or where the physical book is within the holding institution, and this notion is affirmed by the Calisphere "statement on digital primary resources".[https://calisphere.org/overview/] In this statement, these digitzed entries are referred to as primary sources themselves and in their section discussing metadata, they are discussing metadata created from catalouging the physical item rather than from the digital. It seems that this may not have always been the case, given the pilot project which started the Early Modern Annotated Books collection attempted to incorporate elements of the AOR project's XML schema for the original ten books, however this effort seemed to be abandoned once the project moved past the pilot, and even the XML files and transcribed annotations that did exist appear to be no more, with the link provided in the already obscured blog post describing this process being broken.[@palmer_annotated_2018]

Digitization pipeline (only applicable if the dataset is a digitized version of a physical collection):

Who selected what was digitized?
- It is unknown who curated the NLS chapbook collection, both physical and digital
- Since the content of AOR was assembled and digitised for the project, who selected what was digitsed was the same as those whoo curated the collection.
- Philip Palmer selected the first ten books to be digitised for the Clark's Early Modern Annotated Books collection, however it is unclear exactly which ones. Based on those which he discussed in his 2018 blog post, this ten may have included a copy of Sir Thomas Browne's *Pseudodoxia epidemica*, a copy of the 1603 English translation of Montaigne's *Essayes*, Richard Allestree's *The Art of Contentment* (1675), Aleazar Albin's *The Natural History of English Song-Birds* (1779), Sir Richard Blackmore's *Prince Arthur*, and Voltaire's *Dictionnaire Philosophique*.[@palmer_annotated_2018]
What were the steps in the digitization pipeline? (For example, in the case of photos, what scanners were used to digitize the documents? In the case of documents, what OCR engines were utilized?)
- The photos of each chapbook page were captured in the NLS's Causewayside studio by Picturae, a digitization service provider, using a Nikon D800E camera. In the METS data, it is stated that transcriptions associated with each image file were generated from optical character recognition (OCR) performed by the National Library of Scotland. This itself is vague, but looking more deeply at the METS data, it can be seen that the images for each book were combined into a PDF format using a Luratech PDF application, and in the page level METS data, it is stated that `pdfalto`, a command line tool for parsing PDF files and producing structured XML representations of the PDF content in ALTO format, was used.
- AOR-- see above
- Calisphere-- see above
What metadata was algorithmically produced?
- None of the archives explicitly indicate that any of their metadata is algorithimically produced, however the NLS chapbook page transcriptions were created through the use of OCR, and the METS file for this collection were evidentally formatted through some sort of unstated automated process.

Data provenance:

What is the provenance of the dataset, from collection through digitization? (Bender & Friedman, 2018; Diakapoulos et al., n.d.; Holland et al., 2018)
- The process of collecting the images which would make up the training dataset

Crowd labor:
- There was no crowd labour used for this project, however it is important to consider that this project would very much benefit from additional team members, as more images could be annotated and thus a better performing model could be produced.
Have volunteers or crowd workers added metadata to the dataset? (Cordell, 2020; Jakeway et al., 2020; Padilla, 2018)
If so, how were they recruited and compensated?
If so, what metadata did they produce? (i.e., transcriptions, annotations, etc.)

Additional modification:

Were any additional steps taken after collection curation and digitization in order to produce the dataset in question? (i.e., Were any items removed? Were any additional metadata added? etc.)
- There was no additional modifications needed to produce the final dataset. All relevant metadata is present in the project save file created by my application, and since this was...


The machine learning model
Note: if multiple machine learning models were utilized in the project, this step should be completed for each model.

Overview:

{What model architecture has been utilized? (Mitchell et al., 2019) +
What is the task that the model is being deployed to perform? +
Who trained, finetuned, and/or deployed this model? (Mitchell et al., 2019)}
- For the purpose of object detection, I chose to use the YOLOv7 model.[@wang_yolov7] In general, the "You Only Look Once" family of models function by processing the entire image just once to detect objects instead of iteratively analyzing an image multiple times at different scales to identify objects as other object detection models have traditionally done, which makes YOLO models perform the task of object detection faster and more efficiently compared to other models. YOLO models do this by first dividing a picture into a grid of smaller sections, and then for each of these smaller sections, the model tries to predict whether or not there is an object present in that grid, and if so, what kind of object it might be through drawing bounding boxes around the entirety of the possible object and assigning it a label. For each of these boxes drawn, the model will also assign a confidence score, which indicates how sure the model is that an object in that box. If the score is high, it means the model is quite confident, and conversely, if it is low, it is less certain. Lastly, after predicting objects in all of the image's sections, the model eliminates any object that may have multiple bounding boxes drawn around it using a technique called Non-Maximum Suppression, which selects the best bounding boxes by keeping those with the highest confidence score and removing overlap or redundancy.
- Compared to models that break the image down into more granular, pixel-level segmentation, YOLO models' single-pass approach can struggle with smaller objects or complex scenery, however, should these limitations be considered when creating the training dataset, these issues can be countered through methods such as the addition of negative examples or data augmentation as described previously.
Across what organizations or institutions did this training, finetuning, and/or deployment take place? (Mitchell et al., 2019)
What funding was utilized? (Gebru et al., 2020)

Training/finetuning:

Was the model trained from scratch?
If so, what data was used to train the model? (Mitchell et al., 2019)
If not, was a pre-trained model utilized? Where can more information on the pre-trained model be found? (Mitchell et al., 2019)
Was the pre-trained model finetuned? If so, what data was utilized for finetuning?
If training or finetuning was performed, what computational resources were utilized?

Evaluation:

How was the model's performance evaluated? (Mitchell et al., 2019)
What data was used for evaluation? (M. Arnold, Bellamy, et al. (2019); Mitchell et al., 2019)
If the model involves data pertaining to people, has the model been audited for fairness and bias using tools such as FairLearn? (M. Arnold, Bellamy, et al. (2019); Bird et al., 2020; Diakapoulos et al., n.d.; Jakeway et al., 2020; Madaio et al. (2020); Reisman et al., 2018)
Have any tools been utilized to generate explanations for predictions (i.e., LIME Ribeiro et al. (2016), SHAP Lundberg and Lee (2017), TCAV (Kim et al., 2018)) and modify the model in response? (M. Arnold, Bellamy, et al. (2019); Cordell, 2020; Diakapoulos et al., n.d.; Padilla, 2020; Ribeiro et al., 2020)

Deployment:

How was the model deployed? Was it used to make a single pass over the cultural heritage dataset in question, or will it be continuously deployed?
What computational resources were utilized for deployment?
Are the metadata generated by the machine learning model (embeddings, classifications, etc.) available as project deliverables?

Release:

Has the resulting model been made available for download? (if no, the following questions can be skipped)
What license has been provided? (Mitchell et al., 2019)
Who are the primary intended users, and what are the intended use cases? (Mitchell et al., 2019)
Does this model have applicability outside of cultural heritage collections?
What are ways that this model could be misused, either intentionally or unintentionally? (Madaio et al., 2020; Mitchell et al., 2019)

Environmental impact:

What were the carbon emissions produced by training, finetuning, and/or deploying this model? (Cordell, 2020; Lacoste et al., 2019; Strubell et al., 2019)
How does the environmental impact of this model compare to that of other components of the project, such as a collection's digitization or stakeholders’ flights to relevant conferences?

Organizational considerations
Stakeholders:

What stakeholder groups are involved in this project? (Cordell, 2020)
What is each project member's familiarity with machine learning? (Cordell, 2020; Jakeway et al., 2020)
What is each project member's familiarity with cultural heritage collections as data?
Has the project notified and sought input from all potentially relevant stakeholder groups, such as those included within the cultural heritage dataset itself? (Madaio et al., 2020; Reisman et al., 2018)
Do groups affected by the project, such as individuals and communities directly represented within the cultural heritage dataset, have an avenue for contacting project staff and seeking recourse? If so, whom should they contact? If not, why not? (Diakapoulos et al., n.d.; Mitchell et al., 2019; Reisman et al., 2018)

Use of machine learning:

Was it necessary to use machine learning for this project?
If so, why?
If not, why was machine learning still utilized?
What are potential critiques of applying machine learning in this context?

Organizational context:

Can this project be used to build data fluency within the organization or institution? (Padilla, 2020)
Do there exist programs or paths for training staff affiliated with the project to develop machine learning skillsets? (Cordell, 2020; Padilla, 2020)
Do there exist programs or paths for training staff affiliated with the project to develop fluency with cultural heritage collections?

Project deployment and launch:

Who is the target audience of this project? (Madaio et al., 2020)
How does the target audience align with the audiences that the institution or organization is hoping to engage?
If the target audience of the project is the public, does it make an attempt to educate the public regarding the machine learning approaches employed?
Did the project launch reach the intended audience?*
Has the project received feedback from stakeholders, including the audience? If so, what feedback has been received?*
Has the launch of the project resulted in any changes to the project?*

(* = to be completed post-launch)

Copyright, transparency, documentation, maintenance, and privacy
Copyright:

Building on question 1.1.g, does copyright impact the dataset, model, code, or deliverables for the project? (Cordell, 2020; Gebru et al., 2020; Jakeway et al., 2020; Mitchell et al., 2019; Padilla, 2018)
If they are made available, what licenses have been chosen?
If they are proprietary, how does this impact re-use?
Transparency and re-use:

Can the project be audited by outsiders? If so, is there funding available to support outside audits? (Mitchell et al., 2019; Reisman et al., 2018)
Is the code created for the project extensible for other cultural heritage researchers? (Padilla, 2020)
If so, does the project provide any tutorials or toolkits for re-use?

Documentation:

Does the project have documentation? (Katell et al., 2019)
If so, is the documentation interpretable by the project's audience?
Is the project reproducible to an outside researcher, given the documentation available?

Privacy:

If the project is hosted online, are data on visitors collected? If so, what kinds of user data are collected? (Cordell, 2020)
Is visitor consent gained before gathering online data? (Cordell, 2020)

Maintenance:

Will the project and code be maintained? (Gebru et al., 2020)
If so, how frequently, and who will be responsible for maintaining it?