# Case Study: Finding Early Modern Marginalia

DISCUSS: Data sources, data wrangling, annotation process, training model, technical results, historiographical results 

[When regarding collections as data, [we] have discussed the challenges faced relating to metadata, but what about the issues encountered when using collections as data [in their entirety]]? Machine learning models require large-scale data for training, fine-tuning, and evaluation, yet there is a shortage of large-scale datasets within in historical and cultural heritage domains that have slowed this form of techonlogical advancement in this field. Digitized cultural heritage collections can help bridge this gap, however, this requires that the institutions which hold this data be active participants in this shift to making their digtized collections open and accessible to use in ways other than simply viewing.

Cultural heritage instituations are increasingly digitizing their collections and making them available through online portals for public consumption and discovery, the usability of their collections as data is rarely straightforward. Although standardized methods for the virtual exchange of digital collections through the use of Application Programming Interfaces (APIs) is becoming more prevalent, these APIs often lack comprehensive documentation on their usage beyond internal data retrieval. Further, while APIs offer a more formal way of accessing collections as data, many of those who seek to use collections as data find "simple download dumps" more useful to quickly explore what is a collection offers, as they provide direct access to data without the need to learn the intricacies of an API.[@neudecker_ch_2022, 3] Yet even when cultural heritage institutions do offer their data as downloadable content, it often ends up being in complex schema formats like METS, MODS, and ALTO XML, which, though standard in the library and archival domains, pose a barrier for use in contexts of data analysis, where formats such as CSV and JSON files are preferred due to the availability of programming libraries that can easily process these formats. Should the institution be unable to distribute their data in multiple formats, there should be clear documentation on their format of choice, how they are used, and ideally information or links to technical resources that allow transformation into other formats in a straightforward and reproducible way.[@neudecker_ch_2022, 3] To harness the full potential of their content, cultural heritage institutions cannot only rely on the ability of the researchers to access their data through [unmonitored] and time consuming means such as webscraping; instead, they must invest in more suitable ways to share their data, and in digital curation with a considerably broader scope of use, while also [integrating] their responsibilities to the content of their data regarding any ethical issues and inequities that may be present.

As discussed in the context of metadata, the lack of attention paid to the curation of training data in machine learning has been a signficant topic of discussion as of recent, with research emerging from the [field of computer science] on recommendations for ethically sound and transparent standards for publishing datasets alongside calls for more "accountable" curation, as is perceived to be practiced in cultural heritage institutions. These discussions have fed into reflective research within [archival studies] on what ethical issues may arise when using cultural heritage data for the purpose of machine learning. Collections, in both digital and analogue form, are not just sources of history but also "its subjects, sites with histories and politics of their own."[@yale_state_2015, 332] Without criticial reflection on how collections have been curated in past and present, [], or what has and *has not* been digitized, there is the risk of models that are trained using these archives to replicate the epistemologies, injustices, and anxieties exemplified by previous archival orders without action taken toward resolution; even with good intentions, projects using cultural heritage data "risk kitschifying or exploiting those represented in the digitized collections in question."[@bonde_thylstrup_uncertain_2021, 4] [@lee_collections_2023, 2] [A recent demonstration of contested subjects and vulernable histories within digital archives is the Zealy Daguerreotypes], a series of photographs taken by Joseph T. Zealy featuring enslaved men and women in [various states of undress] commissioned by naturalist Louis Agassiz in 1850 as part of his effort to document physical evidence of polygenism. These daguerreotypes were uncovered in the attic of Harvard University's Peabody Museum of Archaeology and Ethnology by an archivist in 1977, and are now featured in the Peabody Museum's online collections, [being, in fact, the first of the search results for "daguerreotype".[https://collections.peabody.harvard.edu/search/daguerreotype/objects]] Tamara Lanier, a descendant of two of Agassiz's "subjects", sued Harvard in 2019 for unlawfully possessing and profiting from the image of her ancestors, who as slaves could not have consented to these photos being taken and their further usage.[@bonde_thylstrup_uncertain_2021, 75] Despite the [contention] surrounding these images, when viewing their entries within the digital archive there is no indication of the history associated with these photos in the metadata, no context given about the enslaved subjects or the fact the photographed individuals were enslaved, nor the photos' purpose as "proof" of polygenesis by Agassiz. There is only a short disclaimer about historical language, which appears to be present on all items in the Peabody Museum's online collections. These individuals are not safe from being commodified again; as images in the public domain, they could easily end up in scraped into a dataset and used for the purpose of [machine learning/image generation like DALLE].

In both an effort to [keep this case study accountable as it is a project which uses cultural heritage collections as data for machine learning], and as a demonstration of how [standardized guidelines] can be implemented and beneficial to research, the remainder of this section will follow the "Collections as ML Data" checklist for machine learning and cultural heritage recently developed by Benjamin Lee.[@lee_collections_2023] Observing the growing trend in the field of machine learning to develop guidelines, checklists, and best practices for researchers and practitioners involved in creating datasets, training models, and implementing machine learning systems, Lee proposes the creation of his "Collections as ML Data" checklist for researchers working on machine learning projects involving cultural heritage collections that is intended to help researchers address potential challenges such as misrepresentation, oversimplification of digitization nuances affecting model performance, unnecessary use of machine learning, lack of sustainability planning, and privacy violations. By incorporating this checklist into their projects, researchers can engage more thoughtfully with these challenges and enhance their impact on the field of digital cultural heritage.[@lee_collections_2023, 2] Lee's article draws from both machine learning and cultural heritage research, creating an interdispilinary tool for responsible data practices. [The checklist in its original form will be attached as an appendix to this project].


## The Cultural Heritage Collection as Data

This section is designed to distinguish "between the cultural heritage collection being studied and the training dataset being utilized for the machine learning model."[@lee_collections_2023, 19] In this case study, I intend to use machine learning to train an object detection model to identify marginalia on pages of the National Library of Scotland's (NLS) collection of chapbooks, and by extension, other early modern texts. In order to create the training data necessary to teach the model what marginalia looks like, I drew from three cultural heritage heritage collections which contained annotated texts created largely during the early modern period, with the oldest printed in the late 15th century and the [occasional text being from the 19th century]. Initially spurring this project was a small selection of annotated pages from the NLS's collection of chapbooks which I had made note of when the marginalia were erranously detected as illustrations during previous work in which I attempted to classify pages in the corpus which contained illustrations. Since the ultimate goal for this case study is to intentionlly identify marginalia in this collection of chapbooks, I chose to incorporate these pages I had made note of into the training data for both diversity and to provide domain specific examples in hopes of improving the trained model's performance. The second digital archive drawn upon is *Archaeology of Reading* project which was a collaborative effort between the Sheridan Libraries, Centre for Editing Lives and Letters, and Princeton University library that resulted in a corpus of thirty-six fully digitized versions of early printed books annotated by "two of the most dedicated readers of the early modern period: John Dee and Gabriel Harvey".[@aoc_2014] Considering himself a scholar of science, the marginalia composed by Dee feature tables, charts, and diagrams to make sense of what he read. In contrast, Harvey was a humanist and approached his selection of reading as such with passages highlighted through underlining and notes connecting the text to other works read. Finally, for increased diversity I drew upon a collection of Early Modern annotated books compiled by the William Andrews Clark Memorial Library at the University of California, Los Angeles which spans from the 15th century up until the early 19th century, much like the NLS's collection of chapbooks. The marginalia of this collection are the most diverse of these sources, containing the scrawl of not only scholars, but printers and other members of the community as well.

### NLS Chapbooks in the Data Foundry
To ensure a complete understanding of what each collection offers researchers as both a subject to be studied and as data, [we] will interrogate each more deeply than just description, beginning with the NLS's chapbooks. NLS Chapbook collection finds its origins within the Lauriston Castle Collection. This collection is a subset of the larger library established by William Robert Reid, a prosperous Edinburgh businessman who acquired Lauriston Castle in 1902.[https://digital.nls.uk/catalogues/special-and-named-printed-collections/?id=598] Reid had been assisted in his book collecting by a family friend, John A Fairley, author of several articles on the bibliography of chapbooks. In the course of his research Fairley had formed a collection of chapbooks containing around 500 volumes comprising over 5,500 items, which are now also part of the collection. The chapbooks are organised according to the town where they were printed, with the assortment mainly consisting of Scottish chapbooks, but English and Irish volumes also contribute to its diversity. The NLS's Digital Scholarship Service is responsible for curating the digitized chapbook dataset which, along with other machine-readable data collections, is made available through the NLS's Data Foundry platform. While specific funding information is not provided, the NLS's Data Foundry operates as an permanant branch of the library, indicating that the resources were drawn from the library's overall budget rather than secured through separate funding endeavors. Further, the item-level data that is viewable after downloading the dataset from the Data Foundry platform explicitly states that, "NLS chapbook's digital form was curated as part of Library activities to make more Scottish collections available online, and chapbooks were selected for this task due to ease as all volumes are the same size."[@chapbooks] 

Although the collection process isn't outlined in detail in the context of digitization, they do offer information about how the chapbooks which were digtized were aquired. The Lauriston Castle Collection was bequeathed to the Library in 1926 by Mr and Mrs Reid, following the latter's death that year. This bequest also included the Reid Fund, consisting of Â£70,000, the income from the estate of Mr and Mrs Reid, which subsequently enabled the Library to acquire printed and manuscript items to add to the national collections.[https://digital.nls.uk/catalogues/special-and-named-printed-collections/?id=598] In terms of the digitization process, on the Data Foundry's listing for the NLS chapbook dataset, there is only a date stamp for its publication in 2019. However, in the item-level data, it states that the chapbooks were captured as part of a project to digitise such materials beginning in 2015. The item-level data further reveals that each page of the chapbooks was captured using a Nikon D800E DSLR camera in the NLS's Causewayside studio by Picturae, a digitization service provider. It is stated that transcriptions associated with each resulting image file were generated from optical character recognition (OCR) performed by the National Library of Scotland. This itself is vague, but looking more deeply at the item-level data, it can be seen that the images for each book were combined into a PDF format using a Luratech PDF application, and in the page-level data, it is stated that `pdfalto`, a command line tool for parsing PDF files and producing structured XML representations of the PDF content in ALTO format, was used.

Despite the straightforward reasoning provided surrounding the NLS's motivations for digitizing the chapbooks and the transparency of the digitization process itself, what remains missing from the process of collection and curation is a question of *who*. The specific individuals responsible for collection and curation decisions are not documented. The identity of those who selected the items for digitization is undisclosed. Additionally, information regarding the original ownership of the chapbooks before Fairley, the cataloging processes pre- and post-digitization, and the individuals involved in transforming the chapbooks into their current digital form within the NLS's Data Foundry remains missing from the available information.

### The Archaeology of Reading

The Archaeology of Reading (AOR), was created as a collaborative research endeavor to consolidate an exemplary portion of the marginalia produced by Gabriel Harvey and John Dee. The assembly of AOR began in November 2015 and was completed in January 2019, with its research and development being conducted with major funding from the Andrew W. Mellon Foundation. The books which make up AOR were selected and annotated by a collective of researchers at the Sheridan Libraries, Centre for Editing Lives and Letters, and Princeton University library, however, the lead researchers Earle Havens, Anthony Grafton, and Lisa Jardine likely had the strongest curatorial role. The largest gap within the AOR collection is that they do not cover what technology was used to capture their texts, although they do indicate that the digitization of the books was done primarily in situ by the repositories who held the physical copies themselves, or through a contract with UCL Digital Media. Further, they indicated that they required the images to have a resolution of 600 DPI, so a DSLR camera like that used by the NLS. While they do not discuess what tools were use specifically, possibly because they themselves did not know due to the geographic expanse of their project, out of all digital collections used for this project, AOR provides the most extensive and transparent description of their collection process, largely in the form of a detailed article on how to "do" AOR yourself, that is, how to replicate their work using a researcher's own corpus, by closely explaining their own process.[@https://archaeologyofreading.org/how-to-do-aor-yourself/] 

AOR is also the most [prolific] when it comes to discussing the curation of their collection, with dedicated essays on the libraries of Harvey and Dee. Although it is thought that Harvey's library once contained up to 4000 books, following his death it was dispersed with his books scattered in private, public, and academic libraries around the world.[https://archaeologyofreading.org/gabriel-harvey-his-library-and-the-aor-corpus/] So, the selection of which of his books to digitized for the AOR project was in large part a practical endeavour; the first books chosen were the nine in the possession of the Princeton University Library, one of their partnering institutions. In addition these nine books, other titles were added to the Harvey AOR corpus often based on factors such as availability (does the binding allow for the book to be digitized?) and the affordability of digitization within a given repository. The Princeton books alone did not form thematic unity, as much as they reflected Harvey's intellectual interests in topics of warfare, (Roman) history, law, political economy (i.e., husbandry), and linguistics. However, the the inclusion of five other titles alongside those at Princeton allowed for the expansion of the topics and the formation of "clusters" of books: books which thematically overlap, and which may have been read in conjunction with one another, as Harvey enjoyed doing. While Dee's library was also dispersed posthumously, and in part, prior to his death due to financial troubles, he created a detailed catalouge of his books at numerous points in his life, which made his pursuits much more easily traceable.[https://archaeologyofreading.org/john-dee-his-library-and-the-aor-corpus/] Like when constructing Harvey's AOR corpus, factors such as the availability of books and the price charged by the various institutions for their digitization were taken into account, however with more choice being present due to the number of identifiable books annotated by Dee, further decision about what of his library should be digitise relied on intellectual interest. It was decided that primarily, the books selected from Dee should comprise of types and styles of reader interventions that are not represented in the Harvey corpus, as Dee's corpus contains a number of new interventions, including the use of additional symbols, genealogical trees, complex astrological charts, dense tables, and expansive drawings. Additionally, in order to further reflect a variety of reading and annotation strategies, the AOR Dee corpus also includes lightly annotated books such as Euclid's *Elementorum libri XV*, as well as different book formats, ranging from Cicero's *Opera* in folio to Gerhard Dorn's *Chymisticum artificium* in octavo. Lastly, in relation to Dee's library as a whole, as in Harvey's corpus, the books included in Dee's corpus were selected to reflect the various intellectual interests which Dee pursued throughout his life, including mathematics, astrology/astronomy, medieval history, and New World discovery. The ultimate goal when curating both corpora was an act of balance, reflecting the attempt to cover a representative selection of both readers' intellectual interests and the ways in which they interacted with their books.

### Early Modern Annotated Books

The Early Modern Annotated Books collection hosted on Calisphere, a digital collections hosting platform for cultural heritage institutions based in Calfornia, was largely curated by the William Andrews Clark Memorial Library (the Clark) which is administered by the University of California (Los Angeles)'s Center for 17th & 18th Century Studies. This rare book and manuscript library specializes in the study of England and the Continent from the Tudor period through the long eighteenth century.[https://calisphere.org/institution/62/collections/] The digitization of the Early Modern Annotated Books collection was initially a 2014 pilot project to digitize just ten annotated books from the Clark library, largely conducted by Philip Palmer who at the time was employed for a CLIR postdoctoral fellowship on the subject of "Manuscript Annotations in Early Modern Printed Books". A small grant from the Gladys Krieble Delmas Foundation allowed the ten books to be transcribed through the hiring of three graduate students onto the project, and the further digitzation of annotated books within the Clark's collections was made possible through funding from the National Endowment for the Humanities, which awarded the library a Humanities Collections and Reference Resources Grant in 2017.[@palmer_annotated_2018]

The collection process of the William Andrews Clark Memorial Library lacks clarity; the metadata is largely bibliographic, and some books include a section on provenance, but it is the provenance of the physical item rather than the digtial. There is no specific information on when the Early Modern Annotated Books collection was assembled in both physical and digital form. Palmer states that the process of digitization began in 2014, however when referencing the funding statement given in the collection's official description, it is implied that the books were captured during the time which the National Endowment for the Humanities grant was held, between 2017 until the project's end in October 2018. There is no clear information provided on the tools used by the Clark for digitization, although looking at the EXIF data extracted by my annotation application, there are tags such as [GPS] common in TIFF images that may point to a camera having been used over technology like a scanner. In contrast to the NLS and AOR, there is also very little information about the decision-making process made in the curation of Early Modern Annotated Books collection. Palmer selected the first ten books to be digitised for the collection based on how they were "representative of the characteristic idiosyncrasy that historical readers brought to their material readings of books", however it is unclear exactly which ten books these are.[@palmer_annotated_2018] Based on those which he discussed in his 2018 blog post on the project, this ten may have included a copy of Sir Thomas Browne's *Pseudodoxia epidemica*, a copy of the 1603 English translation of Montaigne's *Essayes*, Richard Allestree's *The Art of Contentment* (1675), Aleazar Albin's *The Natural History of English Song-Birds* (1779), Sir Richard Blackmore's *Prince Arthur*, and Voltaire's *Dictionnaire Philosophique*.

Evidently, the Clark's Early Modern Annotated Books collection is the collection which leaves the most unknowns, and this seems to be at least in part due to the focus on the digitized books as being surrogates for the physical object, rather than a "new edition".[@cordell_q_2017] The metadata associated with each book appears to be about the physical book or where the physical book is within the holding institution, and this notion is affirmed by the Calisphere "statement on digital primary resources".[https://calisphere.org/overview/] In this statement, these digitzed entries are referred to as primary sources themselves and in the section discussing their metadata, they are discussing metadata created from catalouging the physical item rather than from the digital. It seems that this may not have always been the case, given the pilot project which started the Early Modern Annotated Books collection attempted to incorporate elements of the AOR project's XML schema for the original ten books, however this effort seemed to be abandoned once the project moved past the pilot, and even the XML files and transcribed annotations that did exist appear to be no more, with the link provided in the already obscured blog post describing this process being broken.[@palmer_annotated_2018] The neglect of the pilot project that [originated] this collection is also an issue in itself. There is no evident reference to Palmer's work on developing the project on any [of the institutional] platforms that [define the archive], yet all in-depth information on this collection is derived from a blog post by Palmer written in 2018, which I found by happenstance since this post was a guest publication on the AOR website's blog section. The link to any discussion on how this digital archive came to be is severed to those looking at the collection as it is presented on Calisphere. Further, all additional content crafted by Palmer being no longer available seemingly indicates that the Clark made no effort to preserve these original components of their present collection.

### Dataset Provenance

Although all digital collections used for this case study fell into the public domain and allowed their content to be used for research purposes, neither AOR nor the Clark's Early Modern Annotated Books collection provided their data in an easily downloadable format for researchers who wish to work with their collections computationally. AOR does attempt to offer this via data releases throughout the project's development, however these data releases contain only the project metadata and not the images needed for the purposes of annotation. In consideration of these limitations, I chose to take the common approach of webscraping to collect the images and associated metadata needed to build my training dataset. Webscraping is the automated process of extracting information and data from websites; it involves using digital tools to gather and parse through web pages, collecting data based on parameters set by the [person using the scraper].

Using the Python programming language to write the scripts to perform webscraping, the general structure I followed for webscraping was first, gathering the links to each digitized text's entry, then iterating over each page of the text to extract the image from the webpage and save it to my device. While this approach worked broadly, each archive had its own intricacies that were necessary to adapt my webscraping code to. Since the AOR uses Mirador, an all encompassing viewer for exploring and interacting with digital objects and collections of cultural heritage materials, to display each book in their collection the images are difficult to extract from the webpage's HTML as the typical approach to webscraping does. Thus, to extract the pages from each digitized book, I used the Python library Selenium, which allows for the automation of web browser interactions-- essentially, mimicking the actions a person might to perform a task should it be done manually. In this case, for each book I simulated the process of hitting right-click and "Save As" on each page image, entered what I desired the file name for the downloaded image to be, which in this context consisted of the page number followed by the book label (ex. 15-MattheusBeroaldus-Chronicum(Geneva-1575).jpg), then downloaded the image and hit the arrow button which would lead to the next page, where this process would then repeat until all pages had been downloaded.

At the same time as I saved each image, I would also identify and scrape the metadata entry present for each text.

- As discussed, because the process of annotating images inherently removes them from their [intended] context which can contribute to their misuse, the metadata associated with each page was scraped alongside [the visual representations of the page] and appended as EXIF data so that each page would carry its archival context with it through the process of annotation.
- During the process of scraping and storing the page scans, the metadata associated with the archival item, usually about the book which the page is apart of, was also scraped and embedded within the image using Python's PIL library. Within my annotation app, the EXIF information is also extracted from each image uploaded. This metadata is viewable in-app, but also within the app's save file, which is simply a JSON file.
- NLS --> downloadable METS ALTO format
  - Returning to the discussion Oh of appropriately formatting data for the intended audience, the Data Foundry distributes the metadata for their datasets as METS files at the item-level and ALTO XML files at page-level. Although the information contained within these files is valuable, these formats are very dense and difficult to parse for readers, human and computer alike, attempting to gather information from them. 
  - Harkens back to earlier discussion on good formatting based on intended audience...


What medium is the dataset? (image, video, text, web archive, etc.)
- This dataset is composed of scanned pages of books, extracted from GLAM archives through webscraping
How large is the dataset, both in cardinality and in disk storage?
- In total, 353 images of early modern book pages were annotated. 
- For training, added in to this set of images was 20 [negative examples], that is, pages which contain no marginalia.
- This set of images was then randomly split using an 85/15 training/test ratio, with 317 (~85%) images being used to *train* the model, 39 (~10%) images being used to *validate* the model while it trains, and 17 (~5%) images to test the results of the trained model.
- YOLOv7 recommends 1500 images per new class, so to give this small dataset a fighting chance at successfully detecting marginalia, I applied various transformations to the original training images to create augmented versions of this data. These transformations modified the images in ways that preserve their essential features but introduce variation that can enhance the model's ability to generalize. Not only does data augmentation increase the amount of training data by by generating multiple versions of each original image, but also by presenting the model with different variations of the same image (e.g., different rotations, flips, translations, zoom levels, etc.), data augmentation helps the model learn to recognize important patterns and features that are invariant to those transformations. At the same time, it also helps prevent the model from memorizing specific details of the training data which results in "overfitting"-- [we] want a model that detects meaningful and relevant features rather than memorizing specific examples.
- In the context of this case study, each of the 317 training images were augmented through added noise, randomly changed contrast, randomly changed brightness, and randomly changed saturation. This expanded the total training image set to 1585 (317*4).
- Within the greater context of machine learning, this dataset is very small at only 795 MB.
What metadata is available for the dataset items? (Holland et al., 2018)
- During the process of scraping and storing the page scans, the metadata associated with the archival item, usually about the book which the page is apart of, was also scraped and embedded within the image using Python's PIL library. Within my annotation app, the EXIF information is also extracted from each image uploaded. This metadata is viewable in-app, but also within the app's save file, which is simply a JSON file.
Does copyright impact this dataset? If so, how? (Cordell, 2020; Gebru et al., 2020; Jakeway et al., 2020; Padilla, 2018)
- All data used falls in the public domain and is only being used for research purposes. Further, there appears to be no restrictions from any of the archives indicating that the extracted images cannot be shared in a research context, thus the dataset can be made accessible on an unmonetized platform.
## Annotation Process

## The Machine Learning Model
 
## Results

Drawing upon the description the NLS provides on its chapbook collections, these pocket sized pieces of reading material were printed on a single sheet which was then folded into booklets of 8, 12, 16 and 24 pages, continously produced in this manner from the 17th to 19th century. The subject matter of chapbooks was diverse, with sermons of covenanting ministers, prophecies, last words of murderers, and biographies of famous people such as Wallace, Napoleon and Nelson interspresed with works of humour, fairy tales, poetry, not to mention manuals of instruction and almanacs. It has been estimated that around two thirds of chapbooks contain songs and poems, often under the title garlands. Chapbook printers frequently utilized worn and broken type purchased second-hand which naturally produced rough and unrefined prints; likewise, the woodcuts used to decorate chapbooks were also cycled and reused in print, and often were not at all related to the text they were present in. Chapbooks were sold on streets and at fairs for a penny a time by pedlars dubbed 'chapmen', a term that is related to the word 'cheap' but likely also related to the Anglo-Saxon 'ceapian', meaning to barter, buy and sell. Individuals could also buy them directly from printing shops, although one of the features of chapbooks was the proliferation of provincial imprints with places such as Fintray, Falkirk and Inveraray being a common home to cheap print shops. Chapmen were supported by running stationers to make chapbooks, alongside broadsides, the most popular reading material for the masses during the latter half of the early modern period. Chapbooks gradually disappeared in the mid 19th century due to both the rapidly increasing amount of cheap printed content available and the rise of Victorian morality which considered many chapbook publications as crude and profane.
  
Of the 47329 pages present in the NLS chapbook dataset, a total of 10549 pages were detected as containing marginalia. Given the context of their creation and purpose, as one might have anticipated, the marginalia present in these chapbook is largely "graffiti". There are numerous examples of marks of ownership, with the model uncovering even more of John Watson's signatures alongside new prolific signers, such as William Smitton who neatly placed his mark at the top of each of his chapbooks' title pages. Slightly less common but still notably present is another reader named Peter Smitton, who's signature appears in a similar spot to William's in older chapbooks, revealing perhaps a generation of chapbook consumers. There is also occasionally evidence of female readership, with a reader named Margaret Cameron labelling her books in a manner similar to the Smittons.

[WS, PS, MC]

One categorical oberservation that can be made about these marks broadly is that when comparing the marks of ownership which appear to belong to at children as indicated by their larger and less constrained hand writing, to those of adults, it seems that children often like to assert firm claim upon the chapbook by appending statements such as "his book" or "is my name" to their signature, occasionally alongside a misspelled date. In his research on children's marginalia, scholar Seth Lerer identifies these as "stories of possession"; notes which clearly define who the text belonged to, protecting an object perceived as important by the owner.[@lerer_devotion_2012, 135] Evidence of what appears to be children sharing did however, also show up among the detections, with one page of a chapbook bearing three different names all in different hands and pens, the chapbook seemingly being used as hand writing practice for a group of friends or sibilings.

[1-104184326_107740094.3.jpg + 5-104184636_117859831.3.jpg]

Beyond marks of ownership, there is also evidence of the chapbooks being used for more pragmatic tasks. One of the first detections the model made were of what appears to be tally marks, perhaps someone keeping count of a task or a game or other common repetative task, such as transactions. I propose game primarily, because the tally marks seem to be accompanied by some unusual scrawlings beneath, possibly a rudimentry attempt at spelling by a child. There were also more sophisticated examples of mathematics detected within the chapbook pages, with sequences of of multiplication and addition being performed for an unspecified task; there are examples of similar calculations being performed in the Early Modern Annotated Books collections in the almanac-turned-account book of an 18th century wigmaker, so perhaps these chapbook calculations were also related to business, personal or professional.[https://calisphere.org/item/ark:/21198/n14s4d/] 

[3-104184172_107126626.3.jpg + 2-104185268_108766292.3.jpg]

There were also marginalia detected that demonstrated engagment with the chapbook's text. There are a number of examples of light annotation which might fall under what Grindley classified as "Narrative Reading Aids", with notes clarifying the meaning of words or phrases the reader seemingly did not initially understand; for example, one reader upon coming across the term "Whip whire" wrote below it, "Bird". There are also examples of literary response, primarily in the form of correcting and expanding the chapbook's text. Moreover, graphical responses using systemised forms of graphic shorthand or added punctuation are plentiful, with crosses (+) and x marks being left in places which the reader deemed notable or significant to their understanding of or connection to the text.

[2-104184792_108840536.3.jpg + 2-104186764_108890840.3.jpg]

Observing the chapbook marginalia in its entirety, outside of the content written on the page, an interesting history of the object emerges through a way in which the physical properties of the chapbook's materiality become apparent despite digital format via the writing mediums used by the annotators. In his entry within *Early Modern English Marginalia*, scholar Joshua Calhoun introduces the topic of gelatin sizing, the viscous gelatin solution in which paper was dipped during the early modern period to render it suitable for writing with the water-based ink used for manuscript.[@acheson_early_2019, 19] Conversely, this discussion also brings up the topic of poorly sized paper and "sinking", a contemporary term used to describe paper that could not hold its ink; using porous paper would cause ink to spread, absorb, or run on being applied to it.[@acheson_early_2019, 23] Chapbooks, evidently, were printed on paper that was at most poorly sized, and the marginalia clearly illustrate this. Many marginalia such as that of John Watson's look as if they were written with water colours due to the way the ink spread on the paper, and even the most clear marginalia still suffered from some bleeding along the edges and the occasional blob of ink, obscuring what is written. There are multiple instances of detected marginalia sinking so severely that their meaning is blotted out completely. While chapbooks may seem to be an ideal medium for quick notes and scrap paper given their low cost and proliferation, perhaps such a small percentage of the pages actually contain marginalia in practice since the construct of the paper did not lend itself well to being annotated.

[3-104186661_108901892.3.jpg + 1-104184173_107126446.3.jpg]

Within this collection of images that the model detected as containing marginalia, there were many false positives which is expected given the [middling] evaluation metrics outputted for this model. However, the patterns found within these false positive detections offer clear insight into where the training dataset could be strengthened. Insightfully, the model detected ink smudges from hands, ink bleeding through the pages, ink spills, and printing errors as marginalia, all elements which demonstrate the rough process of production and the human touch which chapbooks underwent in their early lives.

[3-104185187_108783152.3.jpg ----- 4-104184376_117783810.3.jpg + 1-104184376_117783798.3.jpg + 1-104185025_108810164.3.jpg]

Punctuation marks present near the page margins were often detected as marginalia, likely due to the way their placement is similar to the systemised forms of graphic shorthand employed by readers Dee and Harvey, who were both heavily featured in the training data. Additionally, the model extensively detected the woodcut illustrations and page decorations within the chapbooks as marginalia; this is a particularly notable flaw in my dataset, as upon revision of the images annotated, there are very few examples that included illustrations on the page alongside marginalia thus the model never learned to differeniate between these types of "free form" shapes. 


- **Common false detections with low confidence**
  - Ink traces from hands 
  - Ink bleeding through page
  - Ink *spills* --> 4-104184376_117783810.3.jpg + 1-104184376_117783798.3.jpg + 1-104185025_108810164.3.jpg
  - Punctuation marks, specifically near the edge of pages --> likely because these were common additions within training dataset
  - Alllll the illustrations! --> flaw in training dataset, because my training dataset had very minimal examples that included illustrations on the page alongside marginalia, so the model never learned to differeniate between these types of "free form" shapes
    - 1-104184255_108855224.3.jpg
  - Rougher italicized printed characters --> 3-104185490_108497877.3.jpg
  - Printing errors --> 3-104185187_108783152.3.jpg
  - Mold? --> 1-104184967_108514317.3.jpg
- **Correct detections with high confidence!**
  - Tally counts with... bumps at the bottom? --> 3-104184172_107126626.3.jpg
  - Corrections 
    - 3-104184259_117727836.3.jpg
    - 1-104184114_107132470.3.jpg
    - 1-104184114_107132614.3.jpg
    - 1-104184609_117862453.3.jpg
    - 1-104186970_108556377.3.jpg
    - 1-104186974_108555093.3.jpg
    - 2-104184563_107732091.3.jpg --> six foot TWO!
    - 2-104186204_108603861.3.jpg
    - 2-104186498_108932684.3.jpg
    - 2-104186701_108895016.3.jpg --> marginalia itself crossed out?
    - 2-104186764_108890840.3.jpg
  - Reading symbols
    - Pencilled in "+" at certain passages --> 3-104184226_117728712.3.jpg
    - X --> 6-104184266_110322787.3.jpg
    - 1-104184533_117804073.3.jpg
  - Names! aka marks of ownership
    - 3-104184332_107738522.3.jpg Geo smith
    - 3-104184453_117819238.3.jpg C S/Garland
    - 3-104184464_117818194.3.jpg (maybe)
    - 3-104184785_108842240.3.jpg (maybe)
    - 3-104184966_108655149.3.jpg WS
    - 3-104185289_108761552.3.jpg PS
    - 3-104186155_108957200.3.jpg WS
    - 3-104186159_108956048.3.jpg WS
    - 3-104186165_108954512.3.jpg WS
    - 3-104186421_108584229.3.jpg Nelson
    - 3-104186666_108900104.3.jpg WS
    - 4-104186404_108936008.3.jpg John Mitchell and fancy signature ___ Lamb
    - 5-104184636_117859831.3.jpg Multiple readers --> three different names in three different pens  
    - 7-104185577_108731984.3.jpg John _ei_y
    - 1-104184108_107133646.3.jpg Duncan Mc___
    - 1-104184130_107130322.3.jpg Thomas ___ my nam
    - 1-104184176_107125774.3.jpg Margaret Cameson
    - 1-104184326_107740094.3.jpg John Greeg his book agust 17;1753
    - 1-104184338_117794873.3.jpg Sarah Renton [anght] this book 1752
    - 1-104185334_108651021.3.jpg JW his book
    - 1-104185482_108499401.3.jpg WS
    - 1-104186162_108955376.3.jpg WS
    - 1-104186168_108953648.3.jpg WS
    - 1-104186173_108952400.3.jpg John __ooks_
    - 1-104186220_108599157.3.jpg AJ
    - 1-104186661_108901580.3.jpg JW
    - 1-104186661_108902024.3.jpg JW 1234
    - 1-104186661_108902084.3.jpg JW
    - 1-104186662_108901292.3.jpg JW
    - 1-104186662_108901436.3.jpg JW
    - 2-104184130_107130262.3.jpg --> Thomas ___ is my name
    - 2-104184130_107130298.3.jpg Thomas W____
    - 2-104184538_117803089.3.jpg 
    - 2-104184638_117859675.3.jpg Eden ___ ___
    - 2-104184913_108517749.3.jpg Margaret
    - 2-104185019_108811688.3.jpg Margaret
    - 2-104185116_108791048.3.jpg WS
    - 2-104185291_108761168.3.jpg PS
    - 2-104186605_108916952.3.jpg PS
    - 2-104186611_108915608.3.jpg PS
    - 2-104186613_108915224.3.jpg PS
    - 2-104186616_108914360.3.jpg PS
    - 2-104186661_108901592.3.jpg JW
    - 2-104186661_108901724.3.jpg JW
    - 2-104186661_108901904.3.jpg book(?) 59 JW
    - 2-104186662_108901232.3.jpg JW
    - 2-104186664_108900680.3.jpg WS
    - 2-104186665_108900392.3.jpg WS
    - 2-104186909_108875324.3.jpg William Hend Ewan his Ballads 1819
  - Numbers 
    - 3-104184398_117781194.3.jpg
    - 3-104184745_108848768.3.jpg
    - 3-104184757_108847616.3.jpg
    - 3-104184760_108847328.3.jpg
    - 3-104184878_108656301.3.jpg
    - 3-104184909_108518085.3.jpg
    - 3-104184932_108516069.3.jpg
    - 3-104186255_108595413.3.jpg
    - 3-104186694_108896456.3.jpg
    - 3-104186697_108896168.3.jpg
    - 3-104186879_108565005.3.jpg
    - 4-104184885_117873668.3.jpg
    - 1-104184171_107126638.3.jpg
    - 1-104184207_117733095.3.jpg
  - A hand written note pasted(?) onto a page
    - 3-104184787_108841844.3.jpg
    - 4-104184787_108841832.3.jpg
  - Early archival notes
    - 3-104185979_108699452.3.jpg
    - 5-104185187_108783440.3.jpg
    - 6-104185224_108772496.3.jpg
    - 1-104184805_108836552.3.jpg
    - 1-104187060_108682640.3.jpg
    - 2-104184172_107126542.3.jpg
    - 2-104184199_106035047.3.jpg
  - Marks that are so blown out they're illedgable
    - 3-104186186_108608757.3.jpg
    - 1-104184529_117811183.3.jpg
    - 1-104185101_108795080.3.jpg
    - 1-104186147_108609489.3.jpg
    - An interesting history of the object and a way in which the physical properties of the book become apparent despite digital format --> paper sizing and ink quality
  - Edition of copy?
    - 3-104186206_108603189.3.jpg 
    - 3-104186209_108602325.3.jpg
    - 1-104184348_117791259.3.jpg
    - 1-104184546_117825404.3.jpg
    - 1-104184547_117824819.3.jpg
  - Mathematics? 
    - 4-104184395_117781482.3.jpg
    - 1-104184339_117794501.3.jpg
    - 2-104185268_108766292.3.jpg
  - Pen test?
    - 4-104186492_108579789.3.jpg
    - 1-104184494_117804877.3.jpg
    - 1-104186607_108916760.3.jpg
    - 1-104186856_108879740.3.jpg
    - 2-104184552_117823820.3.jpg
    - 2-104184555_117823135.3.jpg
    - 2-104186506_108930860.3.jpg
  - Authorship/literature notes
    - 6-104185152_108652557.3.jpg
    - 6-104185475_108740312.3.jpg
    - 1-104184517_117813655.3.jpg --> date
    - 1-104184896_108519237.3.jpg --> smuggling
    - 1-104185223_108772784.3.jpg
    - 1-104186066_108959336.3.jpg
    - 2-104184643_108522141.3.jpg
    - 2-104184792_108840536.3.jpg
    - 2-104184898_108519045.3.jpg
    - 2-104184906_108518277.3.jpg
    - 2-104185246_108652461.3.jpg
    - 2-104187045_108542625.3.jpg
  - Price six pence --> 2-104184330_107738714.3.jpg
  - Check metadata
    - 3-104186661_108901604.3.jpg
    - 3-104186661_108901892.3.jpg
    - 3-104186661_108902012.3.jpg
    - 3-104186702_108894440.3.jpg
    - 3-104186970_108556221.3.jpg --> Angry note?
    - 4-104185548_108735344.3.jpg
    - 4-104185988_108696848.3.jpg
    - 4-104186147_108609405.3.jpg
    - 4-104186710_108573705.3.jpg
    - 5-104186416_108933272.3.jpg
    - 7-104184636_117859783.3.jpg
    - 13-104184859_108661101.3.jpg
    - 1-104184173_107126446.3.jpg
    - 1-104184192_117739269.3.jpg
    - 1-104184357_117788595.3.jpg
    - 1-104184460_117818578.3.jpg
    - 1-104184701_108853064.3.jpg + 1-104184701_108853088.3.jpg
    - 1-104185059_108800360.3.jpg
    - 1-104185192_108781712.3.jpg
    - 1-104185477_108739832.3.jpg
    - 1-104186019_108690800.3.jpg
    - 1-104186389_108939476.3.jpg
    - 1-104186656_108903320.3.jpg
    - 1-104186825_108888644.3.jpg
    - 2-104184321_104821590.3.jpg
    - 2-104185477_108740036.3.jpg
    - 2-104185482_108499437.3.jpg
    - 2-104186661_108902096.3.jpg
    - 2-104186662_108901160.3.jpg

## Organizational Considerations

## Copyright, Transparency, Documentation, Maintenance, and Privacy


- While writing, was easily able to refer back to the pages annotated and find examples through using the search function

- Sherman discusses origin of marks in *Used Books* pg 27-28

- Confidence score 0.05 since model used without fragmentation of image can miss tiny details if detection not set to be very sensitive
- YOLOv7 recommends 1500 images per new class

------------
## Model
- Explain model

## Data Preprocessing
- In total, 353 images of early modern book pages were annotated. Added in to this set of images was 20 [negative examples], that is, pages which contain no marginalia. Another advantage of the YOLO algorithim is that there is typically little need for negative examples; in the intial step of dividing the inputted image into a grid then [analyzing] each segment, the model will naturally learn how to identify "no object" since a majority of images will not contain objects in every grid segment. However, when the images being used to train the model are "busy" with the objects being detected possibly being present across the entire page, such as with images of traffic or in our case, with pages of text and profilic annotation, training results can be improved through the intentional addition of negative examples to reduce false positives caused by heavy overlap of desired objects and [background] objects. 
- This set of images was then randomly split using the standard 80/20 training/test ratio, with 298 (~80%) images being used to *train* the model, 37 (~10%) images being used to *validate* the model while it trains, and 38 (~10%) images to test the results of the trained model.
- YOLOv7 recommends 1500 images per new class, so to give this small dataset a fighting chance at successfully detecting marginalia, I applied various transformations to the original training images to create augmented versions of this data. These transformations modified the images in ways that preserve their essential features but introduce variation that can enhance the model's ability to generalize. Not only does data augmentation increase the amount of training data by by generating multiple versions of each original image, but also by presenting the model with different variations of the same image (e.g., different rotations, flips, translations, zoom levels, etc.), data augmentation helps the model learn to recognize important patterns and features that are invariant to those transformations. At the same time, it also helps prevent the model from memorizing specific details of the training data which results in "overfitting"-- [we] want a model that detects meaningful and relevant features rather than memorizing specific examples.
- In the context of this case study, each of the 298 training images were augmented through added noise, added gaussian blur, randomly changed contrast, randomly changed brightness, and randomly changed saturation. This expanded the total training image set to 1788 (298*6).

- The following sections will be shaped by Benjamin Charles Germain Lee's recently proposed framework for using collections as machine learning data.[@lee_collections_2023]
- My dataset for this case study was created using three digital archives, ranging from those being made intentionally friendly to computational use to ones following the [standard] archival model, each of which thus emboding the current conversations being had about digital archives as big data.


Data provenance:

What is the provenance of the dataset, from collection through digitization? (Bender & Friedman, 2018; Diakapoulos et al., n.d.; Holland et al., 2018)
- NLS --> downloadable METS ALTO format
  - Harkens back to earlier discussion on good formatting based on intended audience...
- As discussed, because the process of annotating images inherently removes them from their [intended] context which can contribute to their misuse, the metadata associated with each page was scraped alongside [the visual representations of the page] and appended as EXIF data so that each page would carry its archival context with it through the process of annotation.


Crowd labor:
- There was no crowd labour used for this project, however it is important to consider that this project would very much benefit from additional team members, as more images could be annotated and thus a better performing model could be produced.
Have volunteers or crowd workers added metadata to the dataset? (Cordell, 2020; Jakeway et al., 2020; Padilla, 2018)
If so, how were they recruited and compensated?
If so, what metadata did they produce? (i.e., transcriptions, annotations, etc.)

Additional modification:

Were any additional steps taken after collection curation and digitization in order to produce the dataset in question? (i.e., Were any items removed? Were any additional metadata added? etc.)
- There was no additional modifications needed to produce the final dataset. All relevant metadata is present in the project save file created by my application, and since this was...


The machine learning model
Note: if multiple machine learning models were utilized in the project, this step should be completed for each model.

Overview:

{What model architecture has been utilized? (Mitchell et al., 2019) +
What is the task that the model is being deployed to perform? +
Who trained, finetuned, and/or deployed this model? (Mitchell et al., 2019)}
- For the purpose of object detection, I chose to use the YOLOv7 model.[@wang_yolov7] In general, the "You Only Look Once" family of models function by processing the entire image just once to detect objects instead of iteratively analyzing an image multiple times at different scales to identify objects as other object detection models have traditionally done, which makes YOLO models perform the task of object detection faster and more efficiently compared to other models. YOLO models do this by first dividing a picture into a grid of smaller sections, and then for each of these smaller sections, the model tries to predict whether or not there is an object present in that grid, and if so, what kind of object it might be through drawing bounding boxes around the entirety of the possible object and assigning it a label. For each of these boxes drawn, the model will also assign a confidence score, which indicates how sure the model is that an object in that box. If the score is high, it means the model is quite confident, and conversely, if it is low, it is less certain. Lastly, after predicting objects in all of the image's sections, the model eliminates any object that may have multiple bounding boxes drawn around it using a technique called Non-Maximum Suppression, which selects the best bounding boxes by keeping those with the highest confidence score and removing overlap or redundancy.
- Compared to models that break the image down into more granular, pixel-level segmentation, YOLO models' single-pass approach can struggle with smaller objects or complex scenery, however, should these limitations be considered when creating the training dataset, these issues can be countered through methods such as the addition of negative examples or data augmentation as described previously.
- The goal of this model is to detect marginalia present in the NLS chapbooks dataset in order gain a sense of how chapbooks were used by those who interacted with them. It is unlikely that the model will achieve exceptionally high performance due to the small amount of training data, however with YOLOv7's efficient learning capabilities it is likely that the model will achieve a standard of performance that will be acceptable to garner the "bird's eye view" of the chapbooks desired for this experiment.
Across what organizations or institutions did this training, finetuning, and/or deployment take place? (Mitchell et al., 2019)
What funding was utilized? (Gebru et al., 2020)
- The training was performed by myself, a graduate student at Carleton University, and utilized my own funds which are a product of awards and grants from both external and internal sources at Carleton.

Training/finetuning:
- My dataset is small, so rather than training a model from scratch which would likely yield poor results due to this limitation, I chose to use transfer learning with one of YOLOv7's pretrained models. Transfer learning is a machine learning technique where a model that has been trained on one task is repurposed or "finetuned" for a different but related task. Instead of starting from scratch, the knowledge gained from solving one problem is transferred to help solve a different problem; this not only saves time and resources since the model is not being built from the ground up, it also builds upon what the model has already learned about recognizing characteristics such as shapes and patterns, making it more efficient and effective at expanding this [palette].
{Was the model trained from scratch?
If so, what data was used to train the model? (Mitchell et al., 2019)
If not, was a pre-trained model utilized? Where can more information on the pre-trained model be found? (Mitchell et al., 2019)
Was the pre-trained model finetuned? If so, what data was utilized for finetuning?
If training or finetuning was performed, what computational resources were utilized?}
- Under the guidance of the YOLOv7 paper, while also accounting for the techincal parameters of my dataset and hardware being used, I chose the YOLOv7-E6 pretrained model to build off of. The YOLOv7-E6 model is designed for larger input sizes, that is to say, higher resolution images, which helps in capturing smaller details in an image, as well as for use with cloud GPUs.[@wang_yolov7, 6] Cloud GPUs are remote graphics processing units that can be rented or accessed on-demand through cloud computing services for various computational tasks, particularly those involving machine learning workloads since machine learning makes use of the parallel processing capabilities GPUs possess which allow for multiple calculations to be done simultaneously, making complex computation faster and more efficient. In general, cloud GPUs are used because they allow for the use of a more powerful GPU than one typically has on hand, however they can also be [a more environmentally conscious choice]; cloud computing allows for resource sharing among multiple users which reduces the overall energy consumption compared to individuals running their own hardware. To train my model, I chose to use an NVIDIA RTX A6000 from provider Vast.ai, a market-based cloud computing platform which allows all compute providers [large and small] to easily share their devices' spare capacity, due to the high amount of virtual memory which is necessary when training with a higher image resolution, but also because it was available from a Swedish data centre; nearly 75% of electricity production in Sweden comes from renewable, green energy sources, meaning that using a GPU located in Sweden will likely produce less carbon dioxide emissions compared to a GPU present elsewhere.[@https://sweden.se/climate/sustainability/energy-use-in-sweden]

Evaluation:

How was the model's performance evaluated? (Mitchell et al., 2019)
- The key metrics produced upon testing a YOLOv7 model to evaluate its performance are precision, recall, and mean average precision (mAP). Precision is the ratio between actual positive detections and all positive detections; in the context of this model, that would be the measure of marginalia detected on the page out of all the marginalia actually present. Recall indicates how well a model correctly detects [] broadly; thus, for all the marginalia present, recall tells us how many were correctly detected. The mAP compares the bounding box that was drawn by the annotator, the ground-truth bounding box, to the bounding box detected by the model and returns a score; this score determines if an object has been successfully detected or not. YOLOv7 evaluates the model using mAP@.5 specifically; the appended .5 indicates the Intersection-over-Union (IoU) threshold, which measures the minimum overlap between the model's predicted boundary and the ground truth in order for the detection to be considered correct. After training the model for 150 epochs, a single epoch being one pass through the entire training dataset during the training process, when [tested] on the test set of pages the model outputted a precision score of 77.6%, a recall score of 79.5%, and a mAP@.5 of 77.1%. So, when the model predicts marginalia it is generally correct, and the model is able to find and capture a substantial portion of the actual marginalia. Considering the small size of the training dataset and the goals of this project as a whole, these results are [acceptable enough] that the model should provide reasonably accurate detections of marginalia when applied to the NLS chapbook dataset. 

- A low precision and high recall implies that most of the ground-truth objects have been detected, but the majority of the predictions that the model make is incorrect.
  - When a model has high recall but low precision, then the model classifies most of the positive samples correctly but it has many false positives(i.e. classifies many Negative samples as Positive).
  
What data was used for evaluation? (M. Arnold, Bellamy, et al. (2019); Mitchell et al., 2019)
- The `val` set was used for evaluating the model during the process of training, and the `test` set was used for evaluating the model after it completed training.

If the model involves data pertaining to people, has the model been audited for fairness and bias using tools such as FairLearn? (M. Arnold, Bellamy, et al. (2019); Bird et al., 2020; Diakapoulos et al., n.d.; Jakeway et al., 2020; Madaio et al. (2020); Reisman et al., 2018)
- N/A
Have any tools been utilized to generate explanations for predictions (i.e., LIME Ribeiro et al. (2016), SHAP Lundberg and Lee (2017), TCAV (Kim et al., 2018)) and modify the model in response? (M. Arnold, Bellamy, et al. (2019); Cordell, 2020; Diakapoulos et al., n.d.; Padilla, 2020; Ribeiro et al., 2020)
- N/A

Deployment:

How was the model deployed? Was it used to make a single pass over the cultural heritage dataset in question, or will it be continuously deployed?
- This model was primarily an experiment to test the output of my application, thus only intended to make a single pass over the cultural heritage dataset, however the model is available to be used by or even enhanced by others through [].
What computational resources were utilized for deployment?
- N/A
Are the metadata generated by the machine learning model (embeddings, classifications, etc.) available as project deliverables?
- Yes, in the Github repository.

Release:

Has the resulting model been made available for download? (if no, the following questions can be skipped)
- Yes, [].
What license has been provided? (Mitchell et al., 2019)
- 
Who are the primary intended users, and what are the intended use cases? (Mitchell et al., 2019)
- Researchers who would like to reproduce my results, and/or improve upon them.
Does this model have applicability outside of cultural heritage collections?
- N/A
What are ways that this model could be misused, either intentionally or unintentionally? (Madaio et al., 2020; Mitchell et al., 2019)
- If someone did not read this work or [the repository which contains the model] for context, they may assume this model is more capable than it is and apply it in a context which it is inappropriate for, such as to enhance an archival search engine. Should a model like this be used in an institutional setting, higher performance metrics are necessary in order to minimize the risk of false positives or missed objects which would ultimately impair a user's ability to navigate the model's output.

Environmental impact:

{What were the carbon emissions produced by training, finetuning, and/or deploying this model? (Cordell, 2020; Lacoste et al., 2019; Strubell et al., 2019) + 
How does the environmental impact of this model compare to that of other components of the project, such as a collection's digitization or stakeholdersâ flights to relevant conferences?}
- Undoubtly the most resource intensive component of this project was training the model. Although I did not track the carbon emissions of the model as it trained, I utilized the Machine Learning CO2 Impact calculator, which uses the formula of power consumption x time x carbon produced based on the local power grid to estimate the carbon emissions of my training.[@lacoste_quantifying_2019] Since Vast.ai follows a market place shared compute model, I used the Stockholm-based Amazon Web Services datacenter to [stand in for the Swedish datacenter which I rented the RTX A6000 used for training from]. Approximately 15 hours of computation was performed using this GPU, which has a TDP of 300W, between initial attempt at transfer learning and a following attempt after updating hyperparameters for better performance. Region eu-north-1 has a carbon efficiency of 0.05 kgCO$_2$eq/kWh thus the total emissions are estimated to be 0.23 kgCO$_2$eq, which is the equivalent of around 1km driven in a car with an internal combustion engine.
- CALCULATE FOR INFERENCE USING KAGGLE
    

Organizational considerations
Stakeholders:

What stakeholder groups are involved in this project? (Cordell, 2020)
What is each project member's familiarity with machine learning? (Cordell, 2020; Jakeway et al., 2020)
What is each project member's familiarity with cultural heritage collections as data?
Has the project notified and sought input from all potentially relevant stakeholder groups, such as those included within the cultural heritage dataset itself? (Madaio et al., 2020; Reisman et al., 2018)
Do groups affected by the project, such as individuals and communities directly represented within the cultural heritage dataset, have an avenue for contacting project staff and seeking recourse? If so, whom should they contact? If not, why not? (Diakapoulos et al., n.d.; Mitchell et al., 2019; Reisman et al., 2018)

Use of machine learning:

{Was it necessary to use machine learning for this project? + 
If so, why?}
- The alternative to machine learning for this project would be [algorithmic] computer vision, using a series of image augmentations to identify areas of marginalia on a page. However, this approach would not lend well to generalization and be unable to consistently detect marginalia across a diverse dataset since it would not retain information discerned from the images analyzed prior.

If not, why was machine learning still utilized?
What are potential critiques of applying machine learning in this context?
- Is it worth the negative impacts of training?

Organizational context:

Can this project be used to build data fluency within the organization or institution? (Padilla, 2020)
Do there exist programs or paths for training staff affiliated with the project to develop machine learning skillsets? (Cordell, 2020; Padilla, 2020)
Do there exist programs or paths for training staff affiliated with the project to develop fluency with cultural heritage collections?

Project deployment and launch:

Who is the target audience of this project? (Madaio et al., 2020)
- Academics, researchers, those interested in the intersection of cultural heritage institutions and machine learning
How does the target audience align with the audiences that the institution or organization is hoping to engage?
- N/A
If the target audience of the project is the public, does it make an attempt to educate the public regarding the machine learning approaches employed?
- Yes, this writing will be clearly affiliated with the technical output shared, and I have attempted to write the technical components this article with an individual who is unfamiliar with computational concepts and methods in mind.

Did the project launch reach the intended audience?
Has the project received feedback from stakeholders, including the audience? If so, what feedback has been received?*
Has the launch of the project resulted in any changes to the project?

(*â=âto be completed post-launch)

Copyright, transparency, documentation, maintenance, and privacy
Copyright:

Building on question 1.1.g, does copyright impact the dataset, model, code, or deliverables for the project? (Cordell, 2020; Gebru et al., 2020; Jakeway et al., 2020; Mitchell et al., 2019; Padilla, 2018)
- No, all component of this project are free to use for the purpose of research.
If they are made available, what licenses have been chosen?
If they are proprietary, how does this impact re-use?

Transparency and re-use:

Can the project be audited by outsiders? If so, is there funding available to support outside audits? (Mitchell et al., 2019; Reisman et al., 2018)
Is the code created for the project extensible for other cultural heritage researchers? (Padilla, 2020)
- All code used in this project is available for others to freely inspect and use. The webscrapers may be of interest to those who want to collect data from the cultural heritage institutions I referenced, or even as examples of how to scrape similar interfaces. The notebook for training and testing the YOLOv7 model using transfer learning and can be repurposed by others to do the same with their own datasets.
If so, does the project provide any tutorials or toolkits for re-use?
- All code is commented and there is further context for each project element provided in the README of the case study's repository.

Documentation:

Does the project have documentation? (Katell et al., 2019)
- Not formal documentation, however there are notes about use as well as this paper.
If so, is the documentation interpretable by the project's audience?
- Yes
Is the project reproducible to an outside researcher, given the documentation available?
- Yes

Privacy:

If the project is hosted online, are data on visitors collected? If so, what kinds of user data are collected? (Cordell, 2020)
- I do not collect any data about visitors, however GitHub, where repositories related to this project, undoubtly do. From the GitHub Privacy Statement [https://docs.github.com/en/site-policy/privacy-policies/github-privacy-statement]:
> GitHub collects information directly from you for your registration, payment, transactions, and user profile. We also automatically collect from you your usage information, cookies, and device information, subject, where necessary, to your consent. GitHub may also collect personal data from third parties.
Is visitor consent gained before gathering online data? (Cordell, 2020)
- Yes

Maintenance:

{Will the project and code be maintained? (Gebru et al., 2020) +
If so, how frequently, and who will be responsible for maintaining it?}
- The project will be monitored so that should anyone need help with anything related to the project, they may raise an issue on GitHub and will receive a reply. I may develop some aspects of the project further, although this will be done in a repository separate from my MRE, however should this be the case I will update the MRE repository README with a clear link to any further developments.